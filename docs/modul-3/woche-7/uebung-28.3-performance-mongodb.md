---
tags:
  - PostgreSQL
  - Datenbanken
  - MongoDB
  - FastAPI
---
# Performance & MongoDB ‚Äì Praktische √úbungen

## √úbersicht

In dieser √úbung misst du die Performance deiner PostgreSQL-Datenbank und lernst MongoDB als zweites Datenbanksystem kennen:

- **Query-Performance messen** - EXPLAIN ANALYZE nutzen, um Queries zu verstehen
- **Indexes erstellen** - Performance-Optimierung mit B-Tree-Indexes
- **MongoDB installieren** - Lokales Setup der Dokument-Datenbank
- **mongosh CRUD** - Dokumente einf√ºgen, abfragen, aktualisieren und l√∂schen
- **FastAPI + MongoDB** - Events-Endpoint mit motor (async)
- **Polyglot Persistence** - PostgreSQL und MongoDB in einer Anwendung kombinieren

Diese √úbung baut auf Tag 1 (PostgreSQL-Setup, `kursapp`-Datenbank) und Tag 2 (FastAPI-Projekt mit SQLAlchemy) auf.

| Teil | Thema | Zeitbedarf |
|------|-------|------------|
| **R√ºckblick** | Performance & das Dokument-Modell | 10 min (lesen) |
| **Teil 1** | PostgreSQL Performance | 30 min |
| **Teil 2** | MongoDB installieren & CRUD | 30 min |
| **Teil 3** | FastAPI + MongoDB (Polyglot) | 25 min |
| | **Gesamt** | **ca. 2‚Äì2,5 Stunden** |

**Minimalpfad (ca. 90 Minuten):** Teil 1 (PostgreSQL Performance) und Teil 2 (MongoDB Setup & CRUD). Die FastAPI-Integration (Teil 3) und Bonus sind optional, werden aber empfohlen.

---

## R√ºckblick: Performance & das Dokument-Modell

Bevor wir loslegen, ein kurzer R√ºckblick auf die wichtigsten Punkte vom Vormittag.

### Was ist ein Index?

Ein Index ist wie ein **Inhaltsverzeichnis** f√ºr eine Datenbank-Tabelle. Ohne Index muss PostgreSQL jede Zeile einzeln pr√ºfen (**Sequential Scan**). Mit Index findet es den Wert √ºber eine Baumstruktur (**Index Scan**) ‚Äì bei 1 Million Zeilen in ~20 Schritten statt 1 Million Vergleichen.

```mermaid
%%{init: {'theme': 'base', 'themeVariables': {'primaryColor': '#e3f2fd', 'primaryTextColor': '#0d47a1', 'primaryBorderColor': '#90caf9', 'secondaryColor': '#e8f5e9', 'secondaryTextColor': '#1b5e20', 'secondaryBorderColor': '#a5d6a7', 'tertiaryColor': '#fff3e0', 'tertiaryTextColor': '#e65100', 'tertiaryBorderColor': '#ffcc80', 'lineColor': '#78909c', 'fontSize': '14px'}}}%%
graph TD
    R["üîç <b>Root</b><br/>M"] --> B1["üìÇ <b>Branch</b><br/>A‚ÄìL"]
    R --> B2["üìÇ <b>Branch</b><br/>M‚ÄìZ"]
    B1 --> L1["üìÑ <b>Leaf</b><br/>A‚ÄìF ‚Üí Datenseite"]
    B1 --> L2["üìÑ <b>Leaf</b><br/>G‚ÄìL ‚Üí Datenseite"]
    B2 --> L3["üìÑ <b>Leaf</b><br/>M‚ÄìR ‚Üí Datenseite"]
    B2 --> L4["üìÑ <b>Leaf</b><br/>S‚ÄìZ ‚Üí Datenseite"]
```

### Index: Vorteile und Kosten

| Vorteil | Kosten |
|---------|--------|
| SELECTs werden schneller | INSERTs/UPDATEs werden langsamer |
| WHERE, JOIN, ORDER BY profitieren | Jeder Index braucht Speicherplatz |
| Gro√üe Tabellen profitieren am meisten | Bei kleinen Tabellen kaum Unterschied |

**Faustregel:** Index auf Spalten, die oft in WHERE, JOIN oder ORDER BY vorkommen. Nicht auf Spalten, die sehr h√§ufig geschrieben werden.

### Relational vs Dokument

| Konzept | PostgreSQL (Relational) | MongoDB (Dokument) |
|---------|------------------------|-------------------|
| Datencontainer | Table | Collection |
| Eintrag | Row | Document (JSON/BSON) |
| Feld | Column | Field |
| Prim√§rschl√ºssel | Primary Key (uuid) | `_id` (ObjectId) |
| Beziehungen | Foreign Key + JOIN | Embed oder `$lookup` |
| Schema | Fest (ALTER TABLE) | Flexibel (verschiedene Felder pro Dokument) |

### Polyglot Persistence

```mermaid
%%{init: {'theme': 'base', 'themeVariables': {'primaryColor': '#e3f2fd', 'primaryTextColor': '#0d47a1', 'primaryBorderColor': '#90caf9', 'secondaryColor': '#e8f5e9', 'secondaryTextColor': '#1b5e20', 'secondaryBorderColor': '#a5d6a7', 'tertiaryColor': '#fff3e0', 'tertiaryTextColor': '#e65100', 'tertiaryBorderColor': '#ffcc80', 'lineColor': '#78909c', 'fontSize': '14px'}}}%%
graph LR
    R["üñ•Ô∏è <b>React</b><br/>Frontend"] -->|HTTP + JSON| F["‚öôÔ∏è <b>FastAPI</b><br/>Backend"]
    F -->|SQL| PG["üêò <b>PostgreSQL</b><br/>Users, Orders"]
    F -->|BSON| MO["üçÉ <b>MongoDB</b><br/>Events, Logs"]
```

> **Polyglot Persistence** = Verschiedene Datenbanken f√ºr verschiedene Aufgaben. PostgreSQL f√ºr strukturierte Gesch√§ftsdaten (ACID, JOINs), MongoDB f√ºr flexible Event-Daten (Logs, Analytics).

### H√§ufige Fehlannahmen

| Behauptung | Realit√§t |
|-----------|---------|
| "MongoDB ist schneller als PostgreSQL" | Kommt auf die Query an ‚Äì beide k√∂nnen schnell oder langsam sein |
| "NoSQL braucht kein Schema" | Schema-flexibel ‚â† schema-los ‚Äì Validierung ist trotzdem empfohlen |
| "Man muss sich f√ºr eine DB entscheiden" | Polyglot Persistence: verschiedene DBs f√ºr verschiedene Aufgaben |
| "NoSQL ist moderner" | Beide Ans√§tze haben St√§rken ‚Äì die richtige Wahl h√§ngt vom Use-Case ab |

### Wissensfrage 1

Was macht ein Index und warum hat er Kosten?

<details markdown>
<summary>Antwort anzeigen</summary>

**Was er macht:** Ein Index erstellt eine sortierte Datenstruktur (B-Tree), die PostgreSQL erlaubt, Werte schnell zu finden ‚Äì statt alle Zeilen zu durchsuchen (Sequential Scan), springt die DB direkt zur richtigen Stelle (Index Scan).

**Warum er Kosten hat:**
1. **Schreiboperationen werden langsamer:** Bei jedem INSERT, UPDATE oder DELETE muss auch der Index aktualisiert werden.
2. **Speicherplatz:** Der Index braucht zus√§tzlichen Platz auf der Festplatte.
3. **Wartung:** PostgreSQL muss den Index bei vielen √Ñnderungen gelegentlich reorganisieren.

</details>

### Wissensfrage 2

Wie erkennst du in der EXPLAIN-Ausgabe, ob ein Index benutzt wird?

<details markdown>
<summary>Antwort anzeigen</summary>

- **Ohne Index:** Die Ausgabe zeigt `Seq Scan` (Sequential Scan) ‚Äì PostgreSQL pr√ºft jede Zeile.
- **Mit Index:** Die Ausgabe zeigt `Index Scan using idx_name` ‚Äì PostgreSQL nutzt den Index.

Weitere Hinweise:
- `Filter:` = PostgreSQL filtert nach dem Lesen (kein Index f√ºr diese Bedingung)
- `Index Cond:` = PostgreSQL nutzt den Index f√ºr diese Bedingung (schneller)
- `EXPLAIN ANALYZE` zeigt zus√§tzlich die tats√§chliche Ausf√ºhrungszeit (`Execution Time`)

</details>

---

## Teil 1: PostgreSQL Performance

### √úbung 1: Testdaten erzeugen und Query analysieren

> **Ziel:** Testdaten erstellen und eine Query mit EXPLAIN ANALYZE analysieren
> **Zeitbedarf:** ca. 10 Minuten
> **Du bist fertig, wenn:** Du die EXPLAIN-Ausgabe gelesen und die Execution Time notiert hast

Verbinde dich mit deiner `kursapp`-Datenbank:

```bash
psql -h localhost -U postgres -d kursapp
```

**Schritt 1: Testdaten einf√ºgen**

F√ºge 5.000 Test-User ein, damit wir genug Daten f√ºr aussagekr√§ftige Performance-Messungen haben:

```sql
-- Safety Net: Bis PostgreSQL 12 kommt gen_random_uuid() aus pgcrypto.
-- Ab PostgreSQL 13 geht es auch ohne ‚Äì diese Zeile schadet aber nicht.
CREATE EXTENSION IF NOT EXISTS pgcrypto;

INSERT INTO users (id, email, name)
SELECT gen_random_uuid(), 'user' || g || '@example.com', 'User ' || g
FROM generate_series(1, 5000) g
ON CONFLICT DO NOTHING;
```

> **Was passiert hier?** `generate_series(1, 5000)` erzeugt die Zahlen 1 bis 5000. F√ºr jede Zahl wird ein User mit einer eindeutigen Email erstellt. `ON CONFLICT DO NOTHING` √ºberspringt Duplikate.

**Schritt 2: Statistiken aktualisieren**

```sql
ANALYZE users;
```

> **Was macht ANALYZE?** PostgreSQL sammelt Statistiken √ºber die Datenverteilung in der Tabelle. Der Query Planner nutzt diese Statistiken, um den besten Ausf√ºhrungsplan zu w√§hlen.

**Schritt 3: Query analysieren**

```sql
EXPLAIN ANALYZE SELECT * FROM users WHERE name = 'User 42';
```

> **Warum `name` und nicht `email`?** Die Spalte `email` hat bei uns ein `UNIQUE`-Constraint ‚Äì PostgreSQL erstellt daf√ºr automatisch einen Index. Um den Unterschied zwischen "ohne Index" und "mit Index" zu sehen, nehmen wir `name` ‚Äì diese Spalte hat keinen Index.

Du solltest eine Ausgabe wie diese sehen:

```
Seq Scan on users  (cost=0.00..135.00 rows=1 width=64)
  Filter: ((name)::text = 'User 42'::text)
  Rows Removed by Filter: 5002
Planning Time: 0.08 ms
Execution Time: 2.15 ms
```

**Notiere dir:**
- Scan-Typ: `Seq Scan` (Sequential Scan ‚Äì alle Zeilen werden gepr√ºft)
- `Execution Time`: _____ ms

> **Hinweis:** `EXPLAIN` zeigt nur den *geplanten* Ausf√ºhrungsplan. `EXPLAIN ANALYZE` f√ºhrt die Query tats√§chlich aus und zeigt die *echte* Zeit. F√ºr Performance-Messungen immer `EXPLAIN ANALYZE` verwenden.

<details markdown>
<summary>Musterl√∂sung anzeigen</summary>

Die genauen Zeiten variieren je nach Rechner. Wichtig ist:

- Du siehst `Seq Scan` (nicht `Index Scan`) ‚Äì weil `name` keinen Index hat
- Die `Execution Time` liegt typischerweise bei 1‚Äì5 ms
- `Rows Removed by Filter: ~5000` zeigt, dass PostgreSQL alle Zeilen pr√ºfen musste

Nach der √úbung 2 (mit Index auf `name`) wird sich das deutlich verbessern.

</details>

---

### √úbung 2: Index erstellen und Performance vergleichen

> **Ziel:** Einen Index erstellen und den Performance-Unterschied messen
> **Zeitbedarf:** ca. 10 Minuten
> **Du bist fertig, wenn:** EXPLAIN ANALYZE `Index Scan` statt `Seq Scan` zeigt

**Schritt 1: Index auf name erstellen**

```sql
CREATE INDEX idx_users_name ON users (name);
```

**Schritt 2: Gleiche Query nochmal analysieren**

```sql
EXPLAIN ANALYZE SELECT * FROM users WHERE name = 'User 42';
```

Jetzt solltest du sehen:

```
Index Scan using idx_users_name on users  (cost=0.28..8.29 rows=1 width=64)
  Index Cond: ((name)::text = 'User 42'::text)
Planning Time: 0.09 ms
Execution Time: 0.04 ms
```

**Trage deine Ergebnisse ein:**

| | Ohne Index | Mit Index |
|-|-----------|-----------|
| **Scan-Typ** | Seq Scan | Index Scan |
| **Execution Time** | _____ ms | _____ ms |

**Schritt 3: Multi-Column-Index erstellen**

Erstelle einen Index √ºber mehrere Spalten f√ºr die `orders`-Tabelle:

```sql
CREATE INDEX idx_orders_user_status ON orders (user_id, status);
```

> **Multi-Column-Index:** Die Reihenfolge der Spalten ist wichtig! Der Index wird nur genutzt, wenn die erste Spalte (`user_id`) in der WHERE-Bedingung vorkommt. Ein Query mit nur `WHERE status = 'pending'` profitiert nicht von diesem Index.

**Schritt 4: Bestehende Indexes anzeigen**

```sql
\di
```

Du siehst alle Indexes in der Datenbank ‚Äì inklusive der automatisch erstellten (f√ºr Primary Keys und UNIQUE-Constraints).

### Cardinality: Wann lohnt sich ein Index?

| Spalte | Cardinality | Index sinnvoll? |
|--------|------------|-----------------|
| `email` | Hoch (jeder Wert einzigartig) | Ja ‚Äì wird durch UNIQUE automatisch indexiert |
| `name` | Hoch (viele verschiedene Werte) | Ja ‚Äì Index kann gezielt zugreifen |
| `user_id` | Hoch (viele verschiedene User) | Ja ‚Äì bei JOINs und WHERE |
| `created` | Hoch (viele verschiedene Zeitpunkte) | Ja ‚Äì bei ORDER BY und Zeitfiltern |
| `status` | Niedrig (wenige Werte: pending, done) | Oft nicht ‚Äì Index filtert wenig |
| `is_active` | Sehr niedrig (nur true/false) | Nein ‚Äì Sequential Scan oft schneller |

> **Cardinality** = Anzahl verschiedener Werte in einer Spalte. Je h√∂her die Cardinality, desto n√ºtzlicher ist ein Index.

<details markdown>
<summary>Musterl√∂sung anzeigen</summary>

Typische Ergebnisse (variieren je nach System):

| | Ohne Index | Mit Index |
|-|-----------|-----------|
| **Scan-Typ** | Seq Scan | Index Scan |
| **Execution Time** | 1‚Äì5 ms | 0.02‚Äì0.1 ms |

Das ist ein Unterschied von Faktor **20‚Äì100x**! Bei gr√∂√üeren Tabellen (Millionen Zeilen) wird der Unterschied noch dramatischer.

> **Gut zu wissen:** Bei wenigen Zeilen kann PostgreSQL trotz Index einen Seq Scan w√§hlen ‚Äì der kostenbasierte Planner entscheidet, was schneller ist. Das ist kein Fehler! Bei mehr Daten (z.B. `generate_series(1, 50000)` statt 5000) wird der Unterschied deutlicher.

Die `\di`-Ausgabe zeigt:
```
             List of relations
 Schema |         Name          | Type  | Owner    | Table
--------+-----------------------+-------+----------+--------
 public | idx_orders_user_status| index | postgres | orders
 public | idx_users_name        | index | postgres | users
 public | orders_pkey           | index | postgres | orders
 public | users_email_key       | index | postgres | users
 public | users_pkey            | index | postgres | users
```

> Beachte: `users_email_key` wurde automatisch durch das `UNIQUE`-Constraint auf `email` erstellt. `idx_users_name` ist der Index, den du manuell angelegt hast.

</details>

---

### √úbung 3: Query-Patterns testen

> **Ziel:** Verschiedene Query-Patterns mit EXPLAIN ANALYZE testen
> **Zeitbedarf:** ca. 10 Minuten
> **Du bist fertig, wenn:** Du mindestens 3 verschiedene Query-Patterns mit EXPLAIN ANALYZE getestet hast

### Query-Patterns-Referenz

| Pattern | Beispiel | Profitiert von Index? |
|---------|---------|----------------------|
| `WHERE =` | `WHERE name = 'User 42'` | Ja, bei hoher Cardinality |
| `WHERE IN` | `WHERE status IN ('pending', 'done')` | Ja, abh√§ngig von der Werteanzahl |
| `ORDER BY` | `ORDER BY created DESC` | Ja, wenn passender Index existiert |
| `JOIN` | `JOIN users ON users.id = orders.user_id` | Ja, auf FK-Spalte |
| `LIKE 'abc%'` | `WHERE email LIKE 'user1%'` | Ja (oft, Pr√§fix-Suche) |
| `LIKE '%abc'` | `WHERE email LIKE '%example.com'` | **Nein** (Suffix-Suche) |

**Aufgabe:** Teste mindestens 3 dieser Patterns mit EXPLAIN ANALYZE:

```sql
-- Pattern 1: WHERE = (sollte Index nutzen)
EXPLAIN ANALYZE SELECT * FROM users WHERE email = 'user100@example.com';

-- Pattern 2: LIKE Pr√§fix (sollte Index nutzen)
EXPLAIN ANALYZE SELECT * FROM users WHERE email LIKE 'user1%';

-- Pattern 3: LIKE Suffix (kann keinen Index nutzen!)
EXPLAIN ANALYZE SELECT * FROM users WHERE email LIKE '%example.com';

-- Pattern 4: ORDER BY
EXPLAIN ANALYZE SELECT * FROM users ORDER BY email LIMIT 10;
```

> **Beobachte:** Bei welchen Queries siehst du `Index Scan` und bei welchen `Seq Scan`? Die `LIKE '%...'`-Suche (Suffix) kann keinen B-Tree-Index nutzen ‚Äì daf√ºr br√§uchte man einen speziellen Index-Typ.

### Performance-Checkliste

F√ºr den Alltag ‚Äì so optimiert ihr eure Queries:

1. **Indexes pr√ºfen** ‚Äì Gibt es Indexes auf den WHERE/JOIN/ORDER BY-Spalten?
2. **EXPLAIN ANALYZE nutzen** ‚Äì Nicht raten, sondern messen
3. **Auf Seq Scan achten** ‚Äì Bei gro√üen Tabellen ist Seq Scan oft ein Warnsignal
4. **`SELECT *` vermeiden** ‚Äì Nur die Spalten abfragen, die ihr braucht
5. **LIMIT nutzen** ‚Äì Nicht mehr Zeilen laden als n√∂tig
6. **N+1-Problem vermeiden** ‚Äì Nicht in einer Schleife einzelne Queries schicken (lieber JOIN oder IN)

<details markdown>
<summary>Musterl√∂sung anzeigen</summary>

Erwartete Ergebnisse:

| Query | Scan-Typ | Warum? |
|-------|----------|--------|
| `WHERE email = '...'` | Index Scan | Index auf email existiert |
| `LIKE 'user1%'` | Index Scan | Pr√§fix-Suche kann B-Tree nutzen |
| `LIKE '%example.com'` | Seq Scan | Suffix-Suche braucht vollen Tabellenscan |
| `ORDER BY email LIMIT 10` | Index Scan | Index ist bereits sortiert |

</details>

---

## Teil 2: MongoDB installieren & CRUD

### √úbung 4: MongoDB installieren

> **Ziel:** MongoDB Community Edition lokal installieren und mongosh starten
> **Zeitbedarf:** ca. 15 Minuten
> **Du bist fertig, wenn:** `mongosh` den Prompt zeigt und `show dbs` Datenbanken listet

W√§hle die Anleitung f√ºr dein Betriebssystem:

#### Windows

1. √ñffne [mongodb.com/try/download/community](https://www.mongodb.com/try/download/community) und lade den **MSI-Installer** herunter
2. Starte den Installer:
   - W√§hle **Complete** Installation
   - Aktiviere **Install MongoDB as a Service** (empfohlen)
   - MongoDB Compass kannst du optional mitinstallieren
3. Lade zus√§tzlich **mongosh** herunter: [mongodb.com/try/download/shell](https://www.mongodb.com/try/download/shell)
4. Installiere mongosh und stelle sicher, dass es im PATH ist

**√úberpr√ºfung:** √ñffne eine **neue** Eingabeaufforderung:

```bash
mongosh --version
```

> **Hinweis:** Falls `mongosh` nicht gefunden wird, suche im Startmen√º nach "MongoDB Shell" oder f√ºge den Pfad manuell zum PATH hinzu (typisch: `C:\Program Files\mongosh\`).

> **Wichtig (WSL-Nutzer):** MongoDB Community wird in WSL offiziell nicht unterst√ºtzt. Nutze den Windows-Installer (MSI) direkt ‚Äì nicht die Linux-Anleitung in WSL. Docker als Alternative kommt n√§chste Woche.

#### macOS

```bash
brew tap mongodb/brew
brew install mongodb-community@7.0
```

> **Hinweis:** Falls `mongodb-community@7.0` nicht gefunden wird, pr√ºfe die aktuelle Version auf [mongodb.com/docs/manual/tutorial/install-mongodb-on-os-x](https://www.mongodb.com/docs/manual/tutorial/install-mongodb-on-os-x/) und passe die Versionsnummer an.

Server starten:

```bash
brew services start mongodb-community@7.0
```

mongosh ist automatisch dabei. **√úberpr√ºfung:**

```bash
mongosh --version
```

#### Linux (Ubuntu/Debian)

```bash
# MongoDB GPG Key und Repository hinzuf√ºgen
# Version (hier 7.0) ggf. anpassen ‚Äì aktuelle Version auf mongodb.com/docs/manual/installation pr√ºfen
curl -fsSL https://www.mongodb.org/static/pgp/server-7.0.asc | sudo gpg -o /usr/share/keyrings/mongodb-server-7.0.gpg --dearmor
echo "deb [ signed-by=/usr/share/keyrings/mongodb-server-7.0.gpg ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/7.0 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list
sudo apt update
sudo apt install -y mongodb-org
```

Server starten:

```bash
sudo systemctl start mongod
sudo systemctl enable mongod
```

**√úberpr√ºfung:**

```bash
mongosh --version
```

#### Verbindung testen

Starte mongosh:

```bash
mongosh
```

Du solltest einen Prompt sehen wie:

```
Current Mongosh Log ID: 665a1b2c...
Connecting to:          mongodb://127.0.0.1:27017/
Using MongoDB:          x.x.x    ‚Üê (deine installierte Version)
test>
```

Teste die Verbindung:

```javascript
show dbs
```

Du siehst die Standard-Datenbanken (`admin`, `config`, `local`).

### Troubleshooting

| Problem | Ursache | L√∂sung |
|---------|---------|--------|
| `mongosh` nicht gefunden | Nicht im PATH | Startmen√º (Win) oder PATH anpassen |
| `Connection refused` | MongoDB-Dienst l√§uft nicht | Dienst starten (siehe oben) |
| Port 27017 belegt | Anderer Prozess nutzt den Port | Prozess finden: Win `netstat -ano \| findstr 27017`, macOS/Linux `lsof -i :27017` ‚Äì dann blockierenden Prozess beenden |
| Permission denied (Linux) | Fehlende Rechte | `sudo systemctl start mongod` |

**Server-Status pr√ºfen** (l√§uft der Dienst wirklich?):

```bash
# macOS
brew services list

# Linux
systemctl status mongod

# Windows: Win+R ‚Üí services.msc ‚Üí nach "MongoDB Server" suchen
```

---

### √úbung 5: MongoDB CRUD in mongosh

> **Ziel:** Dokumente in MongoDB einf√ºgen, abfragen, aktualisieren und l√∂schen
> **Zeitbedarf:** ca. 10 Minuten
> **Du bist fertig, wenn:** `db.events.find()` deine eingef√ºgten Dokumente zeigt

**Schritt 1: Datenbank wechseln**

```javascript
use kursapp
```

> **Hinweis:** MongoDB erstellt die Datenbank automatisch, sobald du das erste Dokument einf√ºgst. `use kursapp` wechselt zur Datenbank ‚Äì sie existiert noch nicht physisch.

**Schritt 2: Dokument einf√ºgen (insertOne)**

```javascript
db.events.insertOne({
    type: "login",
    user: "max@example.com",
    payload: {
        ip: "192.168.1.42",
        browser: "Chrome"
    },
    ts: new Date()
})
```

> **Beachte:** Das Dokument hat verschachtelte Felder (`payload.ip`, `payload.browser`) und ein Array w√§re auch m√∂glich. MongoDB speichert das als **BSON** (Binary JSON) ‚Äì mit zus√§tzlichen Typen wie `ObjectId`, `Date` und `Decimal128`.

F√ºge noch ein paar Events ein:

```javascript
db.events.insertOne({
    type: "page_view",
    user: "anna@example.com",
    payload: { path: "/dashboard", duration_ms: 1200 },
    ts: new Date()
})

db.events.insertOne({
    type: "login",
    user: "max@example.com",
    payload: { ip: "10.0.0.1", browser: "Firefox" },
    ts: new Date()
})
```

**Schritt 3: Dokumente abfragen (find)**

```javascript
// Alle Events anzeigen
db.events.find()

// Nach Typ filtern
db.events.find({ type: "login" })

// Nach verschachteltem Feld filtern (Punkt-Notation)
db.events.find({ "payload.browser": "Chrome" })
```

> **Schema-flexibel:** Beachte, dass die Events unterschiedliche Felder in `payload` haben (ip+browser vs path+duration_ms). Das ist in MongoDB erlaubt ‚Äì jedes Dokument kann eigene Felder haben. Das hei√üt aber nicht "kein Schema" ‚Äì Validierung ist trotzdem empfohlen!

**Schritt 4: Dokument aktualisieren (updateOne)**

```javascript
db.events.updateOne(
    { user: "max@example.com", type: "login" },
    { $set: { "payload.browser": "Chrome 120" } }
)
```

> **`$set`** √§ndert nur das angegebene Feld ‚Äì der Rest des Dokuments bleibt unver√§ndert.

**Schritt 5: Dokument l√∂schen (deleteOne)**

```javascript
db.events.deleteOne({ type: "page_view" })
```

**√úberpr√ºfung:**

```javascript
db.events.find()
```

Du solltest noch mindestens 2 Dokumente sehen.

<details markdown>
<summary>Musterl√∂sung anzeigen</summary>

Die Ausgabe von `db.events.find()` zeigt Dokumente mit automatisch generierten `_id`-Feldern:

```javascript
[
  {
    _id: ObjectId('665a1b2c3d4e5f6a7b8c9d01'),
    type: 'login',
    user: 'max@example.com',
    payload: { ip: '192.168.1.42', browser: 'Chrome 120' },
    ts: ISODate('2025-06-15T14:30:00.000Z')
  },
  {
    _id: ObjectId('665a1b2c3d4e5f6a7b8c9d03'),
    type: 'login',
    user: 'max@example.com',
    payload: { ip: '10.0.0.1', browser: 'Firefox' },
    ts: ISODate('2025-06-15T14:31:00.000Z')
  }
]
```

Die MongoDB-CRUD-Befehle im Vergleich zu SQL:

| MongoDB | SQL (PostgreSQL) |
|---------|-----------------|
| `db.events.insertOne({...})` | `INSERT INTO events (...) VALUES (...)` |
| `db.events.find({type: "login"})` | `SELECT * FROM events WHERE type = 'login'` |
| `db.events.updateOne({...}, {$set: {...}})` | `UPDATE events SET ... WHERE ...` |
| `db.events.deleteOne({...})` | `DELETE FROM events WHERE ...` |

</details>

### Wissensfrage 3

Was ist der Unterschied zwischen **Embed** und **Reference** in MongoDB?

<details markdown>
<summary>Antwort anzeigen</summary>

**Embed** (Einbetten): Zusammengeh√∂rige Daten werden direkt im Dokument verschachtelt.

```javascript
// Embed: Adresse im User-Dokument
{ "name": "Max", "address": { "street": "Hauptstr. 1", "city": "Berlin" } }
```

**Reference** (Referenzieren): Daten werden in getrennten Collections gespeichert und √ºber IDs verkn√ºpft.

```javascript
// users Collection
{ "name": "Max", "address_id": ObjectId("...") }

// addresses Collection
{ "_id": ObjectId("..."), "street": "Hauptstr. 1", "city": "Berlin" }
```

**Entscheidungshilfe:**

| Kriterium | ‚Üí Embed | ‚Üí Reference |
|-----------|---------|-------------|
| Beziehung | 1:1 oder 1:wenige | 1:viele oder n:m |
| Zugriff | Immer zusammen geladen | Oft einzeln ben√∂tigt |
| Gr√∂√üe | Klein und stabil | Gro√ü oder wachsend |
| Beispiel | User ‚Üí Settings | User ‚Üí Orders |

</details>

---

### √úbung 6: MongoDB Indexes und explain

> **Ziel:** Einen Index in MongoDB erstellen und den Performance-Unterschied messen
> **Zeitbedarf:** ca. 5 Minuten
> **Du bist fertig, wenn:** `explain` den Scan-Typ `IXSCAN` statt `COLLSCAN` zeigt

**Schritt 1: Query ohne Index analysieren**

```javascript
db.events.find({ user: "max@example.com" }).explain("executionStats")
```

Suche in der Ausgabe nach `winningPlan.stage` ‚Äì es sollte `COLLSCAN` zeigen (Collection Scan = alle Dokumente durchsuchen, wie Seq Scan in PostgreSQL).

**Schritt 2: Index erstellen**

```javascript
db.events.createIndex({ user: 1 })
```

> **`1`** = aufsteigend sortiert. `-1` w√§re absteigend.

**Schritt 3: Query mit Index analysieren**

```javascript
db.events.find({ user: "max@example.com" }).explain("executionStats")
```

Jetzt sollte `IXSCAN` (Index Scan) statt `COLLSCAN` erscheinen.

**Schritt 4: Alle Indexes anzeigen**

```javascript
db.events.getIndexes()
```

Du siehst den Standard-Index auf `_id` und deinen neuen Index auf `user`.

<details markdown>
<summary>Musterl√∂sung anzeigen</summary>

Vergleich MongoDB vs PostgreSQL:

| Konzept | PostgreSQL | MongoDB |
|---------|-----------|---------|
| Ohne Index | `Seq Scan` | `COLLSCAN` |
| Mit Index | `Index Scan` | `IXSCAN` |
| Index erstellen | `CREATE INDEX idx ON tbl (col)` | `db.col.createIndex({ field: 1 })` |
| Indexes anzeigen | `\di` | `db.col.getIndexes()` |
| Query analysieren | `EXPLAIN ANALYZE ...` | `.explain("executionStats")` |

Die Ausgabe von `getIndexes()`:

```javascript
[
  { v: 2, key: { _id: 1 }, name: '_id_' },
  { v: 2, key: { user: 1 }, name: 'user_1' }
]
```

</details>

---

## Teil 3: FastAPI + MongoDB (Polyglot)

### √úbung 7: Events-Endpoint mit motor

> **Ziel:** Einen FastAPI-Endpoint erstellen, der Events in MongoDB speichert und abruft
> **Zeitbedarf:** ca. 15 Minuten
> **Du bist fertig, wenn:** POST /events speichert in MongoDB und GET /events gibt die Dokumente zur√ºck

> **Polyglot in Aktion:** Eure App nutzt jetzt PostgreSQL f√ºr Users/Orders (strukturierte Daten mit ACID) und MongoDB f√ºr Events (flexible, schema-freie Logs). Das ist das Polyglot-Persistence-Pattern.

**Schritt 1: motor installieren**

Wechsle in dein `backend/`-Verzeichnis (aus Tag 2) und aktiviere das venv:

```bash
pip install motor
```

> **Was ist motor?** Motor ist der asynchrone MongoDB-Treiber f√ºr Python. Er passt perfekt zu FastAPI, weil FastAPI ebenfalls asynchron arbeitet.

**Schritt 2: MongoDB-Verbindung erstellen**

Erstelle die Datei `app/mongodb.py`:

```python
from motor.motor_asyncio import AsyncIOMotorClient

MONGODB_URL = "mongodb://localhost:27017"

client = AsyncIOMotorClient(MONGODB_URL)
mongodb = client.kursapp
```

> **Tipp f√ºr Produktion:** In einer echten App w√ºrde man `MONGODB_URL` aus einer `.env`-Datei laden (wie `DATABASE_URL` in Tag 2) und den Client im Lifecycle der App starten/stoppen (`app.on_event("startup")` / `"shutdown"`). F√ºr diese √úbung reicht die einfache Variante.

**Schritt 3: Events-Endpoints hinzuf√ºgen**

F√ºge in `app/main.py` die neuen Endpoints hinzu:

```python
from app.mongodb import mongodb


@app.post("/events", status_code=201)
async def create_event(event: dict):
    result = await mongodb.events.insert_one(event)
    return {"id": str(result.inserted_id)}


@app.get("/events")
async def list_events(user: str | None = None):
    query = {"user": user} if user else {}
    events = await mongodb.events.find(query).to_list(100)
    for e in events:
        e["_id"] = str(e["_id"])  # ObjectId ‚Üí String f√ºr JSON
    return events
```

**Was ist neu?**

- `async def` statt `def` ‚Äì motor ist asynchron, daher brauchen wir `await`
- `mongodb.events` ‚Äì greift auf die `events`-Collection in der `kursapp`-Datenbank zu
- `ObjectId ‚Üí String` ‚Äì MongoDB's `_id` ist ein `ObjectId` (kein JSON-Typ). Wir konvertieren mit `str()`, damit FastAPI es als JSON zur√ºckgeben kann
- **Tipp:** Wenn du ein `ts`-Feld beim Insert setzen willst, nutze `from datetime import datetime` und `"ts": datetime.utcnow()` im Request-Body

**Schritt 4: Server starten und testen**

```bash
uvicorn app.main:app --reload
```

√ñffne `http://localhost:8000/docs` und teste:

1. **POST /events** mit Body:
   ```json
   {
     "type": "login",
     "user": "max@example.com",
     "payload": {"ip": "192.168.1.42", "browser": "Chrome"}
   }
   ```

2. **GET /events** ‚Äì zeigt alle Events

3. **GET /events?user=max@example.com** ‚Äì filtert nach User

<details markdown>
<summary>Musterl√∂sung anzeigen</summary>

Die vollst√§ndige `app/mongodb.py`:

```python
from motor.motor_asyncio import AsyncIOMotorClient

MONGODB_URL = "mongodb://localhost:27017"

client = AsyncIOMotorClient(MONGODB_URL)
mongodb = client.kursapp
```

Die Events-Endpoints in `app/main.py`:

```python
from app.mongodb import mongodb

@app.post("/events", status_code=201)
async def create_event(event: dict):
    result = await mongodb.events.insert_one(event)
    return {"id": str(result.inserted_id)}

@app.get("/events")
async def list_events(user: str | None = None):
    query = {"user": user} if user else {}
    events = await mongodb.events.find(query).to_list(100)
    for e in events:
        e["_id"] = str(e["_id"])
    return events
```

Eure App hat jetzt **zwei Datenbanken**:
- PostgreSQL: `/users`, `/orders` (SQLAlchemy, synchron)
- MongoDB: `/events` (motor, asynchron)

</details>

### Wissensfrage 4

Wann w√ºrdest du Embed (einbetten) und wann Reference (verweisen) in MongoDB verwenden?

<details markdown>
<summary>Antwort anzeigen</summary>

**Embed** wenn:
- Die Daten eine 1:1 oder 1:wenige Beziehung haben
- Die Daten immer zusammen geladen werden
- Die eingebetteten Daten klein und stabil sind

**Beispiel:** User mit Settings ‚Üí Settings direkt im User-Dokument einbetten.

**Reference** wenn:
- Die Daten eine 1:viele oder n:m Beziehung haben
- Die Daten oft einzeln abgefragt werden
- Die referenzierten Daten gro√ü sind oder h√§ufig wachsen

**Beispiel:** User mit Orders ‚Üí Orders in eigener Collection, Referenz √ºber `user_id`.

</details>

### Wissensfrage 5

Nenne je zwei Use-Cases, bei denen PostgreSQL und bei denen MongoDB die bessere Wahl w√§re.

<details markdown>
<summary>Antwort anzeigen</summary>

**PostgreSQL ist besser bei:**
1. **Gesch√§ftslogik und Transaktionen** ‚Äì z.B. Bestellungen, Kontobewegungen (ACID-Garantie)
2. **Komplexe Beziehungen** ‚Äì z.B. User ‚Üí Orders ‚Üí Produkte (JOINs √ºber mehrere Tabellen)

**MongoDB ist besser bei:**
1. **Events und Logs** ‚Äì z.B. Login-Events, Analytics-Daten (hoher Schreibdurchsatz, flexible Payloads)
2. **CMS/Content** ‚Äì z.B. Artikel mit variablen Feldern (ein Artikel hat Video, ein anderer Galerie)

**Zusatz:** Beide gleichzeitig nutzen (Polyglot Persistence) ist eine valide und g√§ngige Architektur.

</details>

---

### √úbung 8: Audit-Log Vergleich (Optional)

> **Ziel:** Dasselbe Feature (Audit-Log) in beiden Datenbanken umsetzen und vergleichen
> **Zeitbedarf:** ca. 10 Minuten
> **Du bist fertig, wenn:** Du einen Audit-Log-Eintrag in PostgreSQL und in MongoDB erstellt hast

**PostgreSQL-Variante (JSONB):**

Erstelle in psql die Audit-Log-Tabelle:

```sql
CREATE TABLE audit_log (
    id      uuid PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id uuid REFERENCES users(id),
    action  TEXT NOT NULL,
    payload JSONB,
    ts      TIMESTAMPTZ DEFAULT now()
);

-- Eintrag erstellen (user1@ existiert garantiert aus den Testdaten)
INSERT INTO audit_log (user_id, action, payload)
VALUES (
    (SELECT id FROM users WHERE email = 'user1@example.com'),
    'login',
    '{"ip": "192.168.1.42", "browser": "Chrome"}'::jsonb
);

-- Abfragen
SELECT * FROM audit_log ORDER BY ts DESC LIMIT 10;
```

**MongoDB-Variante:**

In mongosh:

```javascript
db.audit_log.insertOne({
    user_id: "user1@example.com",
    action: "login",
    payload: { ip: "192.168.1.42", browser: "Chrome" },
    ts: new Date()
})

db.audit_log.find({ user_id: "user1@example.com" }).sort({ ts: -1 }).limit(10)
```

> **Vergleich:** Beide Varianten funktionieren. PostgreSQL nutzt `JSONB` f√ºr flexible Payloads innerhalb eines festen Schemas. MongoDB erlaubt von Haus aus flexible Strukturen. F√ºr einen Audit-Log, der nur geschrieben und selten mit JOINs abgefragt wird, kann MongoDB die einfachere L√∂sung sein.

### Wissensfrage 6

Vergleiche die PostgreSQL- und MongoDB-Variante des Audit-Logs: Was sind die Vor- und Nachteile jedes Ansatzes?

<details markdown>
<summary>Antwort anzeigen</summary>

| Aspekt | PostgreSQL (JSONB) | MongoDB |
|--------|-------------------|---------|
| **Schema** | Feste Spalten + flexibles JSONB-Feld | Komplett flexibel |
| **Referenzen** | FK auf `users(id)` ‚Üí Datenintegrit√§t | Kein FK ‚Üí User-ID k√∂nnte ung√ºltig sein |
| **Abfragen** | SQL + JSONB-Operatoren (`->`, `->>`) | Native JSON-Queries |
| **Transaktionen** | ACID-Garantie | M√∂glich, aber weniger √ºblich |
| **Schreiblast** | Gut, aber bei sehr vielen Writes overhead durch FK-Check | Optimiert f√ºr hohen Schreibdurchsatz |

**Fazit:** F√ºr Audit-Logs, die hohen Schreibdurchsatz brauchen und selten mit JOINs abgefragt werden, ist MongoDB oft die einfachere Wahl. Wenn der Audit-Log eng mit Gesch√§ftsdaten verkn√ºpft ist (FK-Integrit√§t wichtig), ist PostgreSQL besser.

</details>

---

## Zusammenfassung

### Was du heute gelernt hast

| Konzept | Beschreibung | Beispiel |
|---------|-------------|---------|
| **Index** | Sortierte Datenstruktur f√ºr schnelle Suche | `CREATE INDEX idx ON users (name)` |
| **EXPLAIN ANALYZE** | Query-Performance messen | `EXPLAIN ANALYZE SELECT ...` |
| **Seq Scan vs Index Scan** | Ohne vs mit Index | 2 ms ‚Üí 0.04 ms |
| **Cardinality** | Anzahl verschiedener Werte | email (hoch) vs status (niedrig) |
| **MongoDB** | Dokument-Datenbank (JSON/BSON) | `db.events.insertOne({...})` |
| **Collection/Document** | MongoDB-√Ñquivalente zu Table/Row | `db.events.find({type: "login"})` |
| **Embed vs Reference** | Einbetten vs Verweisen | 1:1 ‚Üí Embed, 1:n ‚Üí Reference |
| **motor** | Async MongoDB-Treiber f√ºr Python | `AsyncIOMotorClient(...)` |
| **Polyglot Persistence** | Mehrere DBs in einer App | PostgreSQL + MongoDB |

### Performance-Checkliste

1. ‚úÖ Indexes auf WHERE/JOIN/ORDER BY-Spalten pr√ºfen
2. ‚úÖ EXPLAIN ANALYZE nutzen ‚Äì nicht raten, sondern messen
3. ‚úÖ Auf Seq Scan bei gro√üen Tabellen achten
4. ‚úÖ `SELECT *` vermeiden ‚Äì nur ben√∂tigte Spalten abfragen
5. ‚úÖ LIMIT nutzen ‚Äì nicht mehr laden als n√∂tig
6. ‚úÖ N+1-Problem vermeiden ‚Äì JOINs oder IN statt Schleifen-Queries

### Ausblick: Tag 4

Morgen geht es um **AWS RDS ‚Äì Managed PostgreSQL in der Cloud**! Ihr lernt:

- **AWS RDS** ‚Äì PostgreSQL als Managed Service
- **Datenbank-Landschaft** ‚Äì RDS, Aurora, DocumentDB im √úberblick
- **Netzwerk & Security** ‚Äì VPC, Security Groups, √∂ffentlicher Zugang
- **Connection Strings** ‚Äì Von `localhost` auf Cloud umstellen

Stellt sicher, dass euer PostgreSQL- und MongoDB-Setup lokal funktioniert ‚Äì das ist die Voraussetzung f√ºr den Vergleich mit der Cloud.

---

## Bonus

### Bonus 1: Index-Experiment mit gro√üen Datenmengen

F√ºge 50.000 Zeilen ein und miss den Unterschied:

```sql
-- Erst den alten Index l√∂schen (falls vorhanden)
DROP INDEX IF EXISTS idx_users_name;

-- 50.000 Zeilen einf√ºgen
INSERT INTO users (id, email, name)
SELECT gen_random_uuid(), 'bulk' || g || '@example.com', 'Bulk User ' || g
FROM generate_series(1, 50000) g
ON CONFLICT DO NOTHING;

ANALYZE users;

-- Ohne Index messen
EXPLAIN ANALYZE SELECT * FROM users WHERE name = 'Bulk User 25000';

-- Index erstellen
CREATE INDEX idx_users_name ON users (name);

-- Mit Index messen
EXPLAIN ANALYZE SELECT * FROM users WHERE name = 'Bulk User 25000';
```

> Bei 50.000+ Zeilen wird der Unterschied noch deutlicher ‚Äì typischerweise 50‚Äì200x schneller mit Index.

### Bonus 2: MongoDB Aggregation Pipeline

Z√§hle die Anzahl Events pro User:

```javascript
db.events.aggregate([
    { $group: { _id: "$user", count: { $sum: 1 } } },
    { $sort: { count: -1 } }
])
```

> Aggregation Pipelines sind MongoDB's √Ñquivalent zu `GROUP BY` in SQL. Die Daten flie√üen durch mehrere Stufen (`$group`, `$sort`, `$match`, etc.).

---

## Checkliste

Bevor du in den Feierabend gehst ‚Äì hast du alles geschafft?

- [ ] EXPLAIN ANALYZE auf die kursapp-Datenbank ausgef√ºhrt
- [ ] Mindestens einen Index erstellt und den Performance-Unterschied gemessen
- [ ] Du verstehst den Unterschied zwischen Seq Scan und Index Scan
- [ ] Du wei√üt, was Cardinality bedeutet und wann ein Index sinnvoll ist
- [ ] MongoDB ist installiert und mongosh funktioniert
- [ ] Du hast Dokumente in MongoDB eingef√ºgt, abgefragt und aktualisiert (CRUD)
- [ ] Du kennst den Unterschied zwischen Embed und Reference
- [ ] FastAPI `/events`-Endpoint speichert Daten in MongoDB
- [ ] Du kannst erkl√§ren, wann PostgreSQL und wann MongoDB sinnvoll ist

**Alles abgehakt? Dann bist du bereit f√ºr Tag 4!**
