---
tags:
  - AWS
  - EC2
  - S3
  - CLI
---
# AWS Praxisprojekt: Apache Logfile-Upload zu S3

## √úberblick

In diesem Praxisprojekt richtest du einen automatisierten Log-Upload-Service ein. Eine EC2-Instanz mit Apache Webserver sammelt Zugriffslogdateien, die t√§glich komprimiert und automatisch in einen S3-Bucket hochgeladen werden.

### Was ist das Ziel dieses Projekts?

Stell dir vor, du betreibst einen Webserver in der Cloud. Jeden Tag greifen Hunderte oder Tausende Nutzer auf deine Website zu. Der Webserver schreibt jeden Zugriff in Logdateien - wer hat wann welche Seite besucht? Diese Logs sind wertvoll f√ºr:

- **Fehlersuche**: Welche Fehler treten auf?
- **Sicherheit**: Gab es verd√§chtige Zugriffe?
- **Statistik**: Wie viele Besucher hatte ich?
- **Compliance**: Gesetzliche Aufbewahrungspflichten

**Das Problem**: Wenn dein Server ausf√§llt oder neu gestartet wird, sind die Logs weg. Au√üerdem f√ºllen sich die Festplatten mit der Zeit.

**Unsere L√∂sung**: Wir sichern die Logs automatisch jeden Tag in S3 (AWS Cloud-Speicher), wo sie sicher und langfristig aufbewahrt werden.

### Was lernst du dabei?

**AWS-Dienste:**
- **EC2**: Virtuelle Server in der Cloud mieten und verwalten
- **S3**: Unbegrenzt Dateien speichern (Object Storage)
- **IAM**: Sichere Berechtigungen ohne Passw√∂rter

**Linux-Administration:**
- Webserver-Installation und -Konfiguration
- Bash-Scripting f√ºr Automatisierung
- Cron-Jobs f√ºr zeitgesteuerte Aufgaben

**Best Practices:**
- Automatisches Backup
- Sichere Cloud-Authentifizierung
- Infrastruktur als Code

### Wie funktioniert es technisch?

**Architektur:**
```
EC2-Instanz (Ubuntu mit Apache)
    ‚îú‚îÄ> Apache schreibt Logs nach /var/log/apache2/
    ‚îÇ   ‚îú‚îÄ> access.log (wer hat zugegriffen?)
    ‚îÇ   ‚îî‚îÄ> error.log (welche Fehler gab es?)
    ‚îÇ
    ‚îî‚îÄ> Cron-Job l√§uft t√§glich um 23:55 Uhr (UTC)
        ‚îú‚îÄ> 1. Logs in tar.gz packen (komprimieren)
        ‚îú‚îÄ> 2. Upload zu S3 mit AWS CLI
        ‚îú‚îÄ> 3. Alte lokale Archive aufr√§umen
        ‚îî‚îÄ> Erfolg/Fehler ins Log schreiben

S3-Bucket (Cloud-Speicher)
    ‚îî‚îÄ> logs/
        ‚îú‚îÄ> apache-logs-2025-10-24.tar.gz
        ‚îú‚îÄ> apache-logs-2025-10-25.tar.gz
        ‚îî‚îÄ> ... (automatisch gel√∂scht nach 30 Tagen)
```

**Warum ist das besser als manuelles Backup?**
- L√§uft automatisch - du musst nichts tun
- Keine vergessenen Backups
- Logs sind auch bei Server-Ausfall sicher
- Spart Festplattenspeicher auf dem Server
- Skalierbar: Funktioniert f√ºr 1 oder 1000 Server

---

## Voraussetzungen

- Zugang zur AWS Sandbox: https://sandboxes.techstarter.de/
- Windows 11 Rechner
- SSH-Client (OpenSSH in PowerShell oder WSL)
- Grundkenntnisse in Bash-Befehlen
- Zeitaufwand: ca. 45-60 Minuten

**Region:** Wir arbeiten in **eu-central-1 (Frankfurt)**

---

## Schritt 1: S3-Bucket erstellen

### Was ist S3?

**S3 (Simple Storage Service)** ist der Cloud-Speicher von AWS. Stell dir S3 vor wie eine riesige Festplatte im Internet, aber viel besser:

- **Unbegrenzte Kapazit√§t**: Du kannst so viele Dateien hochladen, wie du willst
- **Hochverf√ºgbar**: AWS garantiert 99,999999999% (11 Neunen) Haltbarkeit - deine Daten gehen praktisch nie verloren
- **Skalierbar**: Funktioniert f√ºr 1 KB bis Petabytes
- **Bezahlung**: Du zahlst nur f√ºr den Speicher, den du wirklich nutzt (ca. 0,023 USD pro GB/Monat)

**Wichtige S3-Konzepte:**

1. **Bucket**: Ein Container f√ºr Dateien (wie ein Ordner, aber auf h√∂chster Ebene)
   - Jeder Bucket hat einen global eindeutigen Namen
   - Beispiel: `logs-apache-max-847`

2. **Objekt**: Eine Datei im Bucket
   - Kann ein Foto, Video, Log, Backup, etc. sein
   - Hat einen eindeutigen "Key" (Pfad + Dateiname)
   - Beispiel: `logs/apache-logs-2025-10-24.tar.gz`

3. **Region**: Wo deine Daten physisch gespeichert werden
   - Wir nutzen `eu-central-1` (Frankfurt)
   - Daten verlassen die Region nicht (DSGVO-konform)

### Warum privater Bucket?

Wir machen den Bucket **privat**, weil:
- Logs k√∂nnen sensible Daten enthalten (IP-Adressen, User-Agents)
- Nur unser EC2-Server soll darauf zugreifen k√∂nnen
- √ñffentlicher Zugriff w√§re ein Sicherheitsrisiko

Der Zugriff erfolgt sp√§ter √ºber die **IAM-Rolle** - sicherer als Passw√∂rter!

### Anleitung

1. Melde dich in der AWS Console an (https://sandboxes.techstarter.de/)
2. Wechsle oben rechts zur Region **eu-central-1 (Frankfurt)**
3. Suche in der Suchleiste nach **S3** und klicke darauf
4. Klicke auf den Button **Create bucket** (orange)

### Bucket-Konfiguration

**Bucket-Name:** 
- Trage ein: `logs-apache-deinname-123` 
- Ersetze `deinname` durch deinen Vornamen und `123` durch eine Zufallszahl
- Beispiel: `logs-apache-max-847`
- **Wichtig:** Nur Kleinbuchstaben, Ziffern und Bindestriche erlaubt
- Der Name muss global eindeutig sein (wird weltweit nur einmal vergeben)

**Region:**
- W√§hle **EU (Frankfurt) eu-central-1**

**Block Public Access:**
- Lasse alle H√§kchen gesetzt (Block all public access)
- Wir wollen einen privaten Bucket!

**Bucket Versioning:**
- Lasse auf **Disable** (nicht ben√∂tigt)

**Default encryption:**
- Encryption type: **Server-side encryption with Amazon S3 managed keys (SSE-S3)**
- Belasse alle anderen Einstellungen auf Standard

**Optional - Lifecycle Rule (automatisches L√∂schen alter Logs):**
- Klicke nach dem Erstellen des Buckets auf deinen Bucket
- Gehe zum Tab **Management**
- Klicke **Create lifecycle rule**
- Rule name: `delete-old-logs`
- Rule scope: **Apply to all objects in the bucket**
- Lifecycle rule actions: W√§hle **Expire current versions of objects**
- Days after object creation: `30`
- Best√§tige mit dem H√§kchen bei "I acknowledge..."
- Klicke **Create rule**

**Erkl√§rung:** Logdateien, die √§lter als 30 Tage sind, werden automatisch gel√∂scht, um Speicherkosten zu sparen.

5. Klicke ganz unten auf **Create bucket**

### Checkpoint

**Notiere dir:**
- Deinen vollst√§ndigen Bucket-Namen: `____________________`

**Wichtig:** Lass den Browser-Tab mit der AWS Console ge√∂ffnet, wir brauchen ihn gleich wieder!

---

## Schritt 2: IAM-Rolle f√ºr EC2 erstellen

### Was ist IAM und warum brauchen wir Rollen?

**IAM (Identity and Access Management)** regelt, wer was in AWS darf. Stell dir IAM vor wie einen Sicherheitsdienst, der Ausweise und Berechtigungen verwaltet.

### Das Problem: Wie greift EC2 auf S3 zu?

Unser EC2-Server muss Dateien in den S3-Bucket hochladen. Daf√ºr braucht er Berechtigungen. Es gibt **zwei M√∂glichkeiten**:

**‚ùå Schlechte Methode: Access Keys (Benutzername + Passwort)**
```bash
aws configure
AWS Access Key ID: AKIAIOSFODNN7EXAMPLE
AWS Secret Access Key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
```

**Warum schlecht?**
- Die Keys m√ºssen auf dem Server gespeichert werden (unsicher!)
- Bei Kompromittierung: Angreifer hat vollen Zugriff
- Keys laufen nie ab - m√ºssen manuell rotiert werden
- Schwer zu verwalten bei vielen Servern
- Versehentlich in Git committed? ‚Üí Sicherheitsl√ºcke!

**‚úÖ Gute Methode: IAM-Rolle (tempor√§re Berechtigungen)**

Die IAM-Rolle funktioniert wie ein Dienstausweis:
- EC2-Instanz bekommt eine "Rolle" zugewiesen
- AWS gibt automatisch tempor√§re Credentials (erneuern sich st√ºndlich)
- Keine Credentials auf dem Server gespeichert
- Rolle kann nicht gestohlen werden
- Bei Instanz-Terminierung: Zugriff automatisch weg

**Analogie aus dem echten Leben:**
- Access Keys = Haust√ºrschl√ºssel (kann kopiert/verloren werden)
- IAM-Rolle = Gesichtserkennung (nur du, nicht kopierbar)

### Was ist das Least Privilege Principle?

Wir geben dem Server **nur genau die Rechte, die er braucht** - nicht mehr!

**Unser Server braucht:**
- ‚úÖ Bucket auflisten (`s3:ListBucket`)
- ‚úÖ Dateien hochladen (`s3:PutObject`)

**Unser Server braucht NICHT:**
- ‚ùå Dateien l√∂schen
- ‚ùå Bucket l√∂schen
- ‚ùå Andere Buckets sehen
- ‚ùå EC2-Instanzen starten

Wenn der Server kompromittiert wird, kann der Angreifer **nur** Dateien in diesen einen Bucket hochladen - sonst nichts!

### Anleitung

**Schritt 2: Add permissions**
- Hier erstellen wir eine eigene Policy
- Klicke oben auf **Create policy** (√∂ffnet sich in neuem Tab)

### Policy erstellen

Im neuen Tab:

1. Klicke auf den Tab **JSON**
2. L√∂sche den vorhandenen JSON-Code komplett
3. F√ºge folgenden Code ein (ersetze `<DEIN-BUCKET-NAME>` durch deinen echten Bucket-Namen):

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": ["s3:ListBucket"],
      "Resource": "arn:aws:s3:::<DEIN-BUCKET-NAME>"
    },
    {
      "Effect": "Allow",
      "Action": ["s3:PutObject", "s3:AbortMultipartUpload"],
      "Resource": "arn:aws:s3:::<DEIN-BUCKET-NAME>/*"
    }
  ]
}
```

**Beispiel:** Wenn dein Bucket `logs-apache-max-847` hei√üt, dann:
```json
"Resource": "arn:aws:s3:::logs-apache-max-847"
```
und
```json
"Resource": "arn:aws:s3:::logs-apache-max-847/*"
```

**Was bedeutet diese Policy? (Zeile f√ºr Zeile erkl√§rt)**

```json
"Effect": "Allow"  ‚Üí  Erlaube die folgenden Aktionen
```

**Erster Block (Bucket-Ebene):**
```json
"Action": ["s3:ListBucket"]  ‚Üí  Darf den Inhalt des Buckets auflisten
"Resource": "arn:aws:s3:::logs-apache-max-847"  ‚Üí  Nur f√ºr diesen Bucket
```

**Zweiter Block (Objekt-Ebene):**
```json
"Action": ["s3:PutObject", "s3:AbortMultipartUpload"]  ‚Üí  Darf Dateien hochladen
"Resource": "arn:aws:s3:::logs-apache-max-847/*"  ‚Üí  Alle Objekte im Bucket
```

**Warum `/*` am Ende?**
- Ohne `/*`: Berechtigung f√ºr den Bucket selbst (Container)
- Mit `/*`: Berechtigung f√ºr alle Objekte (Dateien) im Bucket

**Warum `AbortMultipartUpload`?**
- Gro√üe Dateien werden in Teilen hochgeladen
- Falls Upload fehlschl√§gt, muss AWS den Teil-Upload abbrechen k√∂nnen
- Verhindert Speicherm√ºll

4. Klicke **Next**
5. Policy name: `S3-LogUpload-Policy`
6. Description: `Allows EC2 to upload logs to S3 bucket`
7. Klicke **Create policy**
8. Schlie√üe diesen Tab und gehe zur√ºck zum Tab "Create role"

1. Suche in der AWS Console nach **IAM** und klicke darauf
2. Klicke im linken Men√º auf **Roles**
3. Klicke auf **Create role**

### Rolle konfigurieren

**Schritt 1: Select trusted entity**
- Trusted entity type: **AWS service**
- Use case: **EC2**
- Klicke **Next**

### Rolle fertigstellen

1. Klicke auf das Refresh-Symbol neben "Create policy"
2. Suche im Suchfeld nach `S3-LogUpload-Policy`
3. W√§hle die Policy mit einem H√§kchen aus
4. Klicke **Next**

**Schritt 3: Name, review, and create**
- Role name: `EC2-S3-LogUploader`
- Description: `Role for EC2 to upload Apache logs to S3`
- Klicke **Create role**

### Checkpoint

Die IAM-Rolle ist jetzt erstellt! Sie verleiht deiner EC2-Instanz sp√§ter die Berechtigung, Logs in den S3-Bucket hochzuladen - ganz ohne Access Keys.

---

## Schritt 3: Security Group vorbereiten

### Was ist eine Security Group?

Eine **Security Group** ist eine virtuelle Firewall f√ºr deine EC2-Instanz. Sie kontrolliert, welcher Netzwerkverkehr rein und raus darf.

**Analogie aus dem echten Leben:**
Stell dir vor, deine EC2-Instanz ist ein Haus:
- **Security Group** = T√ºrsteher mit Liste
- **Inbound Rules** = Wer darf reinkommen?
- **Outbound Rules** = Wer darf rausgehen?

### Welche Ports brauchen wir?

**Port 22 (SSH):**
- **Wof√ºr?** Damit wir uns per Terminal mit dem Server verbinden k√∂nnen
- **Von wo?** Nur von deiner eigenen IP-Adresse (Sicherheit!)
- **Warum nicht von √ºberall?** Sonst k√∂nnten Hacker versuchen, sich einzuloggen

**Port 80 (HTTP):**
- **Wof√ºr?** Damit Browser auf den Apache-Webserver zugreifen k√∂nnen
- **Von wo?** Von √ºberall (das ist ja der Sinn eines Webservers)
- **Nur f√ºr Demo:** In Produktion w√ºrdest du Port 443 (HTTPS) verwenden

**Was ist mit S3-Verkehr?**
- S3-Zugriff l√§uft √ºber HTTPS (Port 443 ausgehend)
- Outbound-Regeln erlauben standardm√§√üig ALLES ‚Üí kein Problem
- Wir m√ºssen nichts extra konfigurieren

### Anleitung

1. Suche in der AWS Console nach **EC2** und klicke darauf
2. Klicke im linken Men√º auf **Security Groups** (unter Network & Security)
3. Klicke **Create security group**

### Security Group konfigurieren

**Basic details:**
- Security group name: `apache-ssh-access`
- Description: `Allow SSH and HTTP access`
- VPC: Lasse die Default VPC ausgew√§hlt

**Inbound rules:**

Klicke **Add rule** f√ºr jede Regel:

**Regel 1 - SSH:**
- Type: **SSH**
- Protocol: TCP
- Port range: 22
- Source: **My IP** (wird automatisch deine aktuelle IP eintragen)
- Description: `SSH from my IP`

**Regel 2 - HTTP:**
- Type: **HTTP**
- Protocol: TCP
- Port range: 80
- Source: **Anywhere-IPv4** (0.0.0.0/0)
- Description: `HTTP for testing`

**Outbound rules:**
- Lasse die Standard-Regel (All traffic to 0.0.0.0/0)

4. Klicke **Create security group**

### Checkpoint

Die Security Group ist fertig! Sie wird gleich der EC2-Instanz zugewiesen.

---

## Schritt 4: EC2-Instanz starten

### Was ist EC2?

**EC2 (Elastic Compute Cloud)** ist der virtuelle Server-Dienst von AWS. Statt einen physischen Server zu kaufen und im Rechenzentrum aufzustellen, mietest du einen virtuellen Server in der Cloud.

### Warum Cloud statt eigener Server?

**Traditioneller Weg (eigener Server):**
- Hardware kaufen (1.000-10.000 ‚Ç¨)
- Rechenzentrum mieten
- Strom bezahlen (24/7)
- K√ºhlung einrichten
- Bei Ausfall: manuell austauschen
- Upgrade: neue Hardware kaufen

**Cloud-Weg (EC2):**
- Server in 5 Minuten starten
- Bezahlen nur f√ºr Laufzeit (ca. 0,01 ‚Ç¨ pro Stunde f√ºr t2.micro)
- Bei Bedarf gr√∂√üer/kleiner machen
- Bei Ausfall: neue Instanz starten
- Kein Hardware-Management

### Was bedeutet t2.micro?

AWS bietet verschiedene **Instanz-Typen** an. Die Bezeichnung `t2.micro` bedeutet:

- **t2** = Instanz-Familie (T steht f√ºr "burstable", also flexibel)
  - Gut f√ºr Workloads mit wechselnder Last
  - Wie ein Auto mit Turbo: Normalerweise sparsam, bei Bedarf mehr Power
  
- **micro** = Gr√∂√üe
  - 1 vCPU (virtuelle CPU)
  - 1 GB RAM
  - Reicht f√ºr unsere Demo perfekt!

**Andere Gr√∂√üen zum Vergleich:**
- `t2.nano`: 0,5 GB RAM (noch kleiner)
- `t2.small`: 2 GB RAM (doppelt so gro√ü)
- `t2.medium`: 4 GB RAM (4x so gro√ü)
- `m5.large`: 8 GB RAM (andere Familie, mehr Performance)

### Was ist ein AMI (Amazon Machine Image)?

Ein **AMI** ist wie eine Installations-DVD, aber f√ºr virtuelle Server:
- Ubuntu, Windows, Amazon Linux, etc.
- Vorinstallierte Software m√∂glich
- Wir nutzen: **Ubuntu Server 22.04 LTS**

**Warum Ubuntu?**
- Weit verbreitet (viel Dokumentation)
- Kostenlos (Open Source)
- **LTS** = Long Term Support (5 Jahre Updates)
- Gute Package-Verwaltung (apt)

### Was ist ein Key Pair?

Der **Key Pair** ist dein Schl√ºssel zum Server:
- **Privater Schl√ºssel** (.pem Datei) = nur bei dir
- **√ñffentlicher Schl√ºssel** = auf dem Server

**Wie funktioniert SSH mit Keys?**
1. Du verbindest dich: `ssh -i key.pem ubuntu@server`
2. Server schickt verschl√ºsselte Challenge
3. Dein privater Key entschl√ºsselt sie
4. Server pr√ºft: Passt? ‚Üí Zugang erlaubt!

**Warum keine Passw√∂rter?**
- Passw√∂rter k√∂nnen erraten werden (Brute Force)
- SSH-Keys sind 2048-4096 Bit lang ‚Üí praktisch unknackbar
- Kein Passwort-Reset n√∂tig

### Anleitung

1. In der EC2-Console, klicke im linken Men√º auf **Instances**
2. Klicke **Launch instances**

### Instanz konfigurieren

**Name and tags:**
- Name: `apache-log-server`

**Application and OS Images (Amazon Machine Image):**
- Quick Start: **Ubuntu**
- W√§hle **Ubuntu Server 22.04 LTS**
- Architecture: **64-bit (x86)**

**Instance type:**
- W√§hle **t2.micro** (Free tier eligible)

**Key pair (login):**
- Klicke **Create new key pair**
  - Key pair name: `apache-log-key`
  - Key pair type: **RSA**
  - Private key file format: **pem** (f√ºr OpenSSH/WSL) ODER **ppk** (f√ºr PuTTY)
  - Klicke **Create key pair**
- Die Datei wird automatisch heruntergeladen - **speichere sie sicher!**

**Network settings:**
- Klicke **Edit**
- VPC: Lasse Default VPC
- Subnet: No preference
- Auto-assign public IP: **Enable**
- Firewall (security groups): **Select existing security group**
- W√§hle die Security Group `apache-ssh-access` aus

**Configure storage:**
- 8 GiB gp3 (Standard ist ausreichend)

**Advanced details:**
- Scrolle nach unten zu **IAM instance profile**
- W√§hle **EC2-S3-LogUploader** aus der Dropdown-Liste

**Wichtig:** Falls du die Rolle hier vergisst, kannst du sie sp√§ter zuweisen √ºber:
EC2 Console ‚Üí Instanz ausw√§hlen ‚Üí Actions ‚Üí Security ‚Üí Modify IAM role

**Summary:**
- Number of instances: **1**

3. Klicke **Launch instance**
4. Klicke auf **View all instances**
5. Warte, bis der Instance State **Running** ist und Status checks **2/2 checks passed** zeigt (dauert ca. 2-3 Minuten)

### Checkpoint

Die Instanz l√§uft! Du hast gerade einen virtuellen Server in Frankfurt gestartet.

**Notiere dir:**
- Public IPv4 address: `____________________`
- Instance ID (beginnt mit i-): `____________________`

**Verstehe, was passiert ist:**
- AWS hat in Sekundenschnelle einen virtuellen Server erstellt
- Der Server l√§uft jetzt in einem Amazon-Rechenzentrum in Frankfurt
- Du zahlst ab jetzt ca. 0,01 ‚Ç¨ pro Stunde (oder kostenlos im Free Tier)
- Der Server hat eine √∂ffentliche IP ‚Üí ist aus dem Internet erreichbar
- Die IAM-Rolle ist aktiv ‚Üí Server kann auf S3 zugreifen

**üì∏ Screenshot 1 einreichen: EC2-Instanz-Details**

Klicke auf deine Instanz und mache einen Screenshot, der Folgendes zeigt:
- Instance State: **Running** (gr√ºner Punkt)
- Public IPv4 address (sichtbar)
- Security groups: **apache-ssh-access**
- IAM role: **EC2-S3-LogUploader**

Dieser Screenshot beweist: Deine Instanz l√§uft mit der richtigen Konfiguration!

---

## Schritt 5: Mit EC2-Instanz verbinden (SSH)

### Was ist SSH?

**SSH (Secure Shell)** ist ein verschl√ºsseltes Protokoll, um sich mit einem entfernten Server zu verbinden. Es ist wie eine sichere Fernsteuerung f√ºr den Server.

**Was passiert beim SSH-Login?**
1. Dein Computer verbindet sich mit dem Server (Port 22)
2. Server schickt seinen Fingerprint (Identit√§t)
3. Dein privater Key authentifiziert dich
4. Verbindung wird verschl√ºsselt aufgebaut
5. Du siehst die Terminal-Kommandozeile des Servers

**Warum verschl√ºsselt?**
- Alle Daten zwischen dir und dem Server sind verschl√ºsselt
- Niemand kann mitlesen (nicht mal dein ISP)
- Selbst √∂ffentliche WLANs sind sicher

### Option A: PowerShell (Windows)

1. √ñffne PowerShell als Administrator
2. Navigiere zum Ordner mit deinem Key:
```powershell
cd Downloads
```

3. Setze die richtigen Berechtigungen f√ºr den Key (wichtig!):
```powershell
icacls apache-log-key.pem /inheritance:r
icacls apache-log-key.pem /grant:r "$($env:USERNAME):R"
```

4. Verbinde dich mit der EC2-Instanz (ersetze `<PUBLIC-IP>`):
```powershell
ssh -i apache-log-key.pem ubuntu@<PUBLIC-IP>
```

5. Tippe `yes` wenn du nach "Are you sure you want to continue connecting?" gefragt wirst

### Option B: WSL (Windows Subsystem for Linux)

1. √ñffne WSL (Ubuntu)
2. Kopiere den Key von Windows nach WSL:
```bash
cp /mnt/c/Users/<DEIN-USERNAME>/Downloads/apache-log-key.pem ~/
```

3. Setze die richtigen Berechtigungen:
```bash
chmod 400 ~/apache-log-key.pem
```

4. Verbinde dich mit der EC2-Instanz:
```bash
ssh -i ~/apache-log-key.pem ubuntu@<PUBLIC-IP>
```

### Checkpoint

Du solltest jetzt mit der EC2-Instanz verbunden sein und einen Prompt wie diesen sehen:
```
ubuntu@ip-172-31-XX-XX:~$
```

**Was bedeutet dieser Prompt?**
- `ubuntu` = Dein Benutzername auf dem Server
- `ip-172-31-XX-XX` = Hostname des Servers (interne IP)
- `~` = Du bist im Home-Verzeichnis (`/home/ubuntu`)
- `$` = Du bist ein normaler User (nicht root)

**Teste die Verbindung:**
```bash
hostname
whoami
pwd
```

Wenn du diese Ausgaben siehst, hat alles funktioniert!

---

## Schritt 6: Apache und AWS CLI installieren

### Was ist Apache?

**Apache HTTP Server** ist der weltweit meistgenutzte Webserver (seit 1995!). Er nimmt HTTP-Anfragen entgegen und liefert Webseiten aus.

**Wie funktioniert ein Webserver?**
```
Browser          ‚Üí  HTTP-Request   ‚Üí  Apache Webserver
(dein PC)           "GET /"            (EC2-Instanz)
                                       ‚îú‚îÄ> Datei suchen (/var/www/html/index.html)
                                       ‚îî‚îÄ> Log schreiben (/var/log/apache2/access.log)
                ‚Üê  HTTP-Response  ‚Üê
                   HTML-Seite
```

**Was wird geloggt?**
Jeder Zugriff wird in `access.log` gespeichert:
```
93.184.216.34 - - [24/Oct/2025:15:42:18 +0000] "GET / HTTP/1.1" 200 10918
```

Das bedeutet:
- `93.184.216.34` = IP des Besuchers
- `24/Oct/2025:15:42:18` = Zeitpunkt
- `GET /` = Anfrage zur Startseite
- `200` = Erfolg (HTTP-Statuscode)
- `10918` = Gr√∂√üe der Antwort in Bytes

**Warum sind Logs wichtig?**
- Fehlersuche ("Warum funktioniert X nicht?")
- Sicherheit ("Wurde ich angegriffen?")
- Statistiken ("Wie viele Besucher?")
- Compliance (gesetzliche Aufbewahrungspflicht)

### Was ist die AWS CLI?

Die **AWS CLI (Command Line Interface)** ist ein Tool, um AWS-Dienste per Terminal zu steuern.

**Beispiel-Befehle:**
```bash
aws s3 ls                          # Alle Buckets auflisten
aws s3 cp file.txt s3://bucket/    # Datei hochladen
aws ec2 describe-instances         # EC2-Instanzen anzeigen
```

**Warum CLI statt Console (Browser)?**
- Automatisierung (in Skripten)
- Schneller (keine Klickerei)
- Wiederholbar (exakt reproduzierbar)
- F√ºr Cron-Jobs (l√§uft ohne Mensch)

### Anleitung

F√ºhre folgende Befehle auf der EC2-Instanz aus (du bist per SSH verbunden):

1. System-Pakete aktualisieren:
```bash
sudo apt update && sudo apt upgrade -y
```

2. Apache und AWS CLI installieren:
```bash
sudo apt install -y apache2 unzip curl
```

3. Apache-Status √ºberpr√ºfen:
```bash
sudo systemctl status apache2
```

Du solltest sehen: `Active: active (running)`

Dr√ºcke `q` um den Status zu verlassen.

4. Apache-Version anzeigen:
```bash
apache2 -v
```

5. AWS-CLI Installationspaket herunterladen:
```bash
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
```

6. Archiv entpacken:
```bash
unzip awscliv2.zip
```

7. Installation starten:
```bash
sudo ./aws/install
```

8. AWS CLI Version anzeigen:
```bash
aws --version
```

9. Test: Apache Webserver aufrufen (vom EC2 selbst):
```bash
curl http://localhost
```

Du solltest HTML-Code sehen, der mit `<!DOCTYPE html>` beginnt.

10. Test: Apache Webserver von au√üen aufrufen
- √ñffne einen Browser auf deinem Windows-PC
- Gehe zu: `http://<PUBLIC-IP-DEINER-EC2>`
- Du solltest die Standard-Ubuntu-Apache-Seite sehen

11. Logdateien √ºberpr√ºfen:
```bash
ls -lh /var/log/apache2/
```

Du solltest mindestens diese Dateien sehen:
- `access.log` (Zugriffslogs)
- `error.log` (Fehlerlogs)

12. Ein paar Zugriffe erzeugen (damit wir Logs haben):
```bash
for i in {1..20}; do curl -s http://localhost > /dev/null; done
```

13. Logs anschauen:
```bash
sudo tail -n 10 /var/log/apache2/access.log
```

Du siehst jetzt die letzten 10 Zugriffe mit IP, Zeit, angeforderter Seite und Status-Code!

### Checkpoint

**Verstehe, was du erreicht hast:**
- ‚úÖ Apache Webserver l√§uft und ist √∂ffentlich erreichbar
- ‚úÖ AWS CLI ist installiert und hat durch die IAM-Rolle automatisch Zugriff auf S3
- ‚úÖ Logdateien werden erstellt und gef√ºllt
- ‚úÖ Alles bereit f√ºr das Backup-Skript!

**üì∏ Screenshot 2 einreichen: Apache l√§uft**

Mache **einen Screenshot**, der zwei Dinge zeigt:

**Teil 1 - Terminal:**
Zeige die Ausgabe von:
```bash
apache2 -v
aws --version
sudo systemctl status apache2
```
(Apache-Version, AWS-CLI-Version, Status "active")

**Teil 2 - Browser:**
√ñffne einen zweiten Browser-Tab und gehe zu `http://<DEINE-PUBLIC-IP>`
Zeige die Apache-Standard-Seite ("Apache2 Ubuntu Default Page")

**Tipp:** Beide Fenster nebeneinander anordnen oder zwei separate Screenshots machen.

---

## Schritt 7: Upload-Skript erstellen

### Was macht das Skript?

Unser Bash-Skript automatisiert drei Aufgaben:

**1. Archivierung (tar + gzip)**
```bash
tar -czf backup.tar.gz /var/log/apache2/
```
- **tar** = Tape Archive (fasst mehrere Dateien zusammen)
- **-c** = create (neu erstellen)
- **-z** = gzip (komprimieren)
- **-f** = file (Ausgabedatei)

**Warum tar.gz und nicht zip?**
- **tar.gz** = Standard unter Linux
- Bessere Kompression (10-30% kleiner)
- Erh√§lt Datei-Berechtigungen
- Unterst√ºtzt gro√üe Dateien besser

**2. Upload zu S3**
```bash
aws s3 sync /local/ s3://bucket/
```
- Synchronisiert Ordner mit S3
- L√§dt nur neue/ge√§nderte Dateien hoch
- Spart Zeit und Traffic

**3. Aufr√§umen**
```bash
find /archive/ -mtime +14 -delete
```
- L√∂scht lokale Archive √§lter als 14 Tage
- Spart Festplattenspeicher auf EC2
- Archive bleiben in S3 (bis Lifecycle-Regel greift)

### Warum als root ausf√ºhren?

```bash
sudo -s
```

Wir wechseln zu root, weil:
- `/var/log/apache2/` geh√∂rt root (normale User k√∂nnen nicht lesen)
- Cron-Jobs mit root-Rechten sind einfacher zu konfigurieren
- Skript muss in `/opt/` schreiben (System-Ordner)

**Sicherheitshinweis:** In Produktion w√ºrdest du einen Service-User mit minimalen Rechten nutzen. F√ºr unsere Demo ist root ok.

### Anleitung

Alle folgenden Befehle werden als root-User ausgef√ºhrt:

1. Wechsle zu root:
```bash
sudo -s
```

Dein Prompt sollte sich √§ndern zu: `root@ip-...#`

2. Arbeitsordner erstellen:
```bash
mkdir -p /opt/logsync /var/log/apache2-archive
```

3. Skript erstellen (wir erstellen es Schritt f√ºr Schritt):
```bash
nano /opt/logsync/upload_apache_logs.sh
```

Der nano-Editor √∂ffnet sich. F√ºge folgenden Inhalt ein:

**Hinweis:** Die `--exclude` Optionen im tar-Befehl verhindern, dass bereits komprimierte Dateien (z.B. durch logrotate) nochmals gezippt werden. Das spart Platz und Upload-Traffic.

```bash
#!/usr/bin/env bash
set -euo pipefail

# WICHTIG: Trage hier deinen Bucket-Namen ein!
BUCKET="DEIN-BUCKET-NAME"

SRC_DIR="/var/log/apache2"
ARCHIVE_DIR="/var/log/apache2-archive"
STAMP="$(date +%Y-%m-%d)"

# 1) Tagesarchiv erzeugen (inkl. aller aktuellen & rotierten Apache-Logs)
echo "[$(date '+%Y-%m-%d %H:%M:%S')] Starting log archive creation..."
tar -czf "${ARCHIVE_DIR}/apache-logs-${STAMP}.tar.gz" -C "${SRC_DIR}" .
echo "[$(date '+%Y-%m-%d %H:%M:%S')] Archive created: apache-logs-${STAMP}.tar.gz"

# 2) Nach S3 synchronisieren
echo "[$(date '+%Y-%m-%d %H:%M:%S')] Uploading to S3..."
aws s3 sync "${ARCHIVE_DIR}/" "s3://${BUCKET}/logs/" --only-show-errors
echo "[$(date '+%Y-%m-%d %H:%M:%S')] Upload completed"

# Alternative: Nur die heutige Datei hochladen (statt sync)
# aws s3 cp "${ARCHIVE_DIR}/apache-logs-${STAMP}.tar.gz" "s3://${BUCKET}/logs/"

# 3) Lokale Archive √§lter als 14 Tage l√∂schen
find "${ARCHIVE_DIR}" -type f -name 'apache-logs-*.tar.gz' -mtime +14 -delete
echo "[$(date '+%Y-%m-%d %H:%M:%S')] Cleanup completed"
```

**WICHTIG:** Ersetze `DEIN-BUCKET-NAME` mit deinem echten Bucket-Namen!

Beispiel: Wenn dein Bucket `logs-apache-max-847` hei√üt, dann:
```bash
BUCKET="logs-apache-max-847"
```

4. Speichern in nano:
- Dr√ºcke `Strg + O` (WriteOut)
- Dr√ºcke `Enter` (best√§tigen)
- Dr√ºcke `Strg + X` (Exit)

5. Skript ausf√ºhrbar machen:
```bash
chmod +x /opt/logsync/upload_apache_logs.sh
```

6. √úberpr√ºfen, ob das Skript ausf√ºhrbar ist:
```bash
ls -lh /opt/logsync/upload_apache_logs.sh
```

Du solltest sehen: `-rwxr-xr-x` (die `x` bedeuten "executable")

7. Skript-Inhalt anzeigen (zur Kontrolle):
```bash
cat /opt/logsync/upload_apache_logs.sh
```

√úberpr√ºfe, ob der Bucket-Name korrekt eingetragen ist!

### Verstehe das Skript (Zeile f√ºr Zeile)

Lass uns das Skript durchgehen und jede Zeile verstehen:

```bash
#!/usr/bin/env bash
```
**Shebang:** Sagt dem System "f√ºhre dieses Skript mit Bash aus"

```bash
set -euo pipefail
```
**Fehlerbehandlung:**
- `-e` = Exit bei Fehler (stoppt bei erstem Problem)
- `-u` = Exit bei undefined variables (f√§ngt Tippfehler)
- `-o pipefail` = Fehler in Pipes beachten

```bash
BUCKET="logs-apache-max-847"
```
**Konfiguration:** Dein S3-Bucket-Name (WICHTIG: anpassen!)

```bash
STAMP="$(date +%Y-%m-%d)"
```
**Datumsstempel:** Erzeugt Format `2025-10-24`
- Sortierbar (√§lteste Datei zuerst)
- Eindeutig pro Tag

```bash
tar -czf "${ARCHIVE_DIR}/apache-logs-${STAMP}.tar.gz" \
  -C "${SRC_DIR}" . \
  --exclude='*.gz' \
  --exclude='*.old'
```
**Archivierung:**
- `-C` = Change directory (erst wechseln, dann packen)
- `.` = Alle Dateien im aktuellen Ordner
- `--exclude` = Bereits komprimierte Dateien auslassen

```bash
aws s3 sync "${ARCHIVE_DIR}/" "s3://${BUCKET}/logs/" --only-show-errors
```
**Upload:**
- `sync` = Synchronisiert Ordner
- `--only-show-errors` = Keine Ausgabe bei Erfolg (weniger Log-M√ºll)

```bash
find "${ARCHIVE_DIR}" -type f -name 'apache-logs-*.tar.gz' -mtime +14 -delete
```
**Cleanup:**
- `find` = Suche Dateien
- `-type f` = Nur normale Dateien (keine Ordner)
- `-name` = Passendes Muster
- `-mtime +14` = √Ñlter als 14 Tage
- `-delete` = L√∂schen

### Checkpoint

Das Skript ist fertig! Es kann jetzt:
- ‚úÖ Logs archivieren
- ‚úÖ Nach S3 hochladen
- ‚úÖ Alte Archive aufr√§umen
- ‚úÖ Fehler protokollieren

---

## Schritt 8: Skript manuell testen

Bevor wir den Cron-Job einrichten, testen wir das Skript manuell.

### Anleitung

Du bist immer noch als root angemeldet (Prompt: `root@ip-...#`)

1. Skript ausf√ºhren:
```bash
/opt/logsync/upload_apache_logs.sh
```

Du solltest folgende Ausgaben sehen:
```
[2025-10-24 XX:XX:XX] Starting log archive creation...
[2025-10-24 XX:XX:XX] Archive created: apache-logs-2025-10-24.tar.gz
[2025-10-24 XX:XX:XX] Uploading to S3...
[2025-10-24 XX:XX:XX] Upload completed
[2025-10-24 XX:XX:XX] Cleanup completed
```

2. √úberpr√ºfe, ob das Archiv erstellt wurde:
```bash
ls -lh /var/log/apache2-archive/
```

Du solltest eine Datei wie `apache-logs-2025-10-24.tar.gz` sehen.

3. √úberpr√ºfe die Gr√∂√üe des Archivs:
```bash
du -h /var/log/apache2-archive/apache-logs-*.tar.gz
```

4. √úberpr√ºfe den Inhalt des Archivs (ohne es zu entpacken):
```bash
tar -tzf /var/log/apache2-archive/apache-logs-*.tar.gz | head -n 20
```

Du solltest Dateien wie `access.log`, `error.log` etc. sehen.

### Checkpoint

**Perfekt!** Das Skript hat funktioniert. Was ist passiert?

1. ‚úÖ Logs wurden in `.tar.gz` gepackt (10-50% der Originalgr√∂√üe)
2. ‚úÖ Archiv wurde nach S3 hochgeladen
3. ‚úÖ IAM-Rolle hat funktioniert (keine Access Keys n√∂tig!)
4. ‚úÖ Alles wurde protokolliert

**üì∏ Screenshot 3 einreichen: Upload erfolgreich**

Mache einen Screenshot, der beweist, dass alles funktioniert hat:

**AWS Console - S3:**
1. Gehe zu S3 ‚Üí dein Bucket ‚Üí logs/
2. Du solltest eine Datei sehen: `apache-logs-2025-10-24.tar.gz`
3. Screenshot sollte zeigen:
   - Bucket-Namen
   - Ordner `logs/`
   - Die .tar.gz-Datei
   - Datum und Uhrzeit des Uploads
   - Dateigr√∂√üe

Dieser Screenshot ist der Beweis: Dein automatisches Backup funktioniert!

---

## Schritt 9: Cron-Job einrichten (Automatisierung)

### Was ist Cron?

**Cron** ist der Standard-Task-Scheduler unter Linux (seit 1970er Jahren!). Er f√ºhrt Befehle zu bestimmten Zeiten automatisch aus.

**Beispiel-Anwendungen:**
- T√§gliches Backup um 23:00 Uhr
- St√ºndliche Website-√úberpr√ºfung
- W√∂chentliche Datenbank-Bereinigung
- Monatliche Rechnung generieren

**Wie funktioniert Cron?**
```
Cron-Daemon (im Hintergrund)
    ‚îú‚îÄ> Pr√ºft jede Minute: "Ist es Zeit f√ºr einen Job?"
    ‚îú‚îÄ> Liest Cron-Dateien in /etc/cron.d/
    ‚îî‚îÄ> F√ºhrt f√§llige Jobs aus
```

### Warum /etc/cron.d/ statt crontab?

Es gibt mehrere Wege, Cron-Jobs zu definieren:

**Methode 1: User-Crontab** (nicht genutzt)
```bash
crontab -e  # Bearbeitet Cron-Jobs f√ºr aktuellen User
```
- Nachteil: L√§uft als User, nicht als root
- Kann nicht auf /var/log/apache2/ zugreifen

**Methode 2: /etc/cron.d/** (unsere Wahl!) ‚úÖ
- System-weite Cron-Jobs
- Kann User angeben (`root`)
- √úbersichtlich (eine Datei pro Task)
- √úberlebt System-Updates

### Cron-Syntax verstehen

Die Zeitangabe in Cron sieht kryptisch aus, ist aber logisch:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Minute (0 - 59)
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Stunde (0 - 23)
‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Tag des Monats (1 - 31)
‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Monat (1 - 12)
‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Wochentag (0 - 7, 0 und 7 = Sonntag)
‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ
* * * * * Befehl

55 23 * * *   ‚Üí  Jeden Tag um 23:55 Uhr
0 */2 * * *   ‚Üí  Alle 2 Stunden
0 0 * * 0     ‚Üí  Jeden Sonntag um Mitternacht
30 6 1 * *    ‚Üí  Jeden 1. des Monats um 06:30
```

**Warum 23:55 Uhr?**
- Nachts wenig Traffic ‚Üí Server-Last gering
- Vor Mitternacht ‚Üí Logs des aktuellen Tages sind komplett
- Nicht genau Mitternacht ‚Üí vermeidet "Mitternachts-Rush" (viele Jobs laufen oft um 00:00)

### Anleitung

Du bist immer noch als root auf der EC2-Instanz.

1. Cron-Job-Datei erstellen:
```bash
nano /etc/cron.d/apache-logsync
```

2. F√ºge folgenden Inhalt ein:
```
SHELL=/bin/bash
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

# T√§glich um 23:55 Uhr (UTC): Logs archivieren und hochladen
55 23 * * * root /opt/logsync/upload_apache_logs.sh >> /var/log/logsync.log 2>&1

```

**Wichtig:** Die Uhrzeit ist in **UTC** (koordinierte Weltzeit), da EC2-Instanzen standardm√§√üig auf UTC laufen.
- 23:55 UTC = 01:55 Uhr deutsche Zeit (im Sommer)
- 23:55 UTC = 00:55 Uhr deutsche Zeit (im Winter)

**Erkl√§rung der Cron-Syntax:**
- `55` = Minute (55)
- `23` = Stunde (23 Uhr)
- `* * *` = jeden Tag, jeden Monat, jeden Wochentag
- `root` = wird als root-User ausgef√ºhrt
- `>> /var/log/logsync.log 2>&1` = Ausgabe und Fehler in Logdatei schreiben

**Aktuelle UTC-Zeit anzeigen:**
```bash
date -u
```

3. Speichern in nano:
- Dr√ºcke `Strg + O` (WriteOut)
- Dr√ºcke `Enter` (best√§tigen)
- Dr√ºcke `Strg + X` (Exit)

**Wichtig:** Die Datei muss mit einer Leerzeile enden (siehe oben im Code-Block)

4. Cron-Job verifizieren:
```bash
cat /etc/cron.d/apache-logsync
```

5. √úberpr√ºfe, dass die Datei mit einer Newline endet:
```bash
tail -c 1 /etc/cron.d/apache-logsync | od -An -tx1
```
Sollte `0a` ausgeben (= Newline vorhanden)

6. Cron-Job-Datei muss die richtigen Berechtigungen haben:
```bash
chmod 644 /etc/cron.d/apache-logsync
ls -l /etc/cron.d/apache-logsync
```
Sollte zeigen: `-rw-r--r--` (= 644 Berechtigungen)

7. Cron-Dienst neu laden (nicht unbedingt n√∂tig, aber sicher ist sicher):
```bash
systemctl restart cron
```

8. Cron-Status √ºberpr√ºfen:
```bash
systemctl status cron
```

Du solltest sehen: `Active: active (running)`

Dr√ºcke `q` zum Beenden.

### Cron-Job testen (ohne zu warten)

Da wir nicht bis 23:55 Uhr warten wollen, √§ndern wir den Cron-Job tempor√§r f√ºr einen Test:

1. Aktuelle Uhrzeit anzeigen:
```bash
date
```

Beispiel-Ausgabe: `Fri Oct 24 15:42:30 UTC 2025`

2. Cron-Job anpassen (f√ºr Test in 2 Minuten):
```bash
nano /etc/cron.d/apache-logsync
```

√Ñndere die Zeit-Zeile. Wenn es jetzt z.B. 15:42 Uhr ist, setze:
```
44 15 * * * root /opt/logsync/upload_apache_logs.sh >> /var/log/logsync.log 2>&1
```

(2 Minuten sp√§ter als aktuelle Uhrzeit)

3. Speichern: `Strg + O` ‚Üí `Enter` ‚Üí `Strg + X`

4. Warte 2-3 Minuten

5. Logdatei √ºberpr√ºfen:
```bash
tail -n 50 /var/log/logsync.log
```

Du solltest die Ausgabe deines Skripts sehen mit Zeitstempeln.

6. Pr√ºfe, ob ein neues Archiv erstellt wurde:
```bash
ls -lt /var/log/apache2-archive/ | head -n 5
```

7. Pr√ºfe S3 (du solltest jetzt 2 Dateien haben):
```bash
aws s3 ls s3://DEIN-BUCKET-NAME/logs/
```

### Cron-Job zur√ºck auf 23:55 Uhr setzen

1. √ñffne die Cron-Datei wieder:
```bash
nano /etc/cron.d/apache-logsync
```

2. √Ñndere zur√ºck auf:
```
55 23 * * * root /opt/logsync/upload_apache_logs.sh >> /var/log/logsync.log 2>&1
```

3. Speichern und beenden

### Checkpoint - Der gro√üe Moment!

Du hast soeben **vollst√§ndige Automatisierung** erreicht! Ab jetzt:

- ‚úÖ L√§uft das Backup t√§glich um 23:55 Uhr (UTC) ohne dein Zutun
- ‚úÖ Logs werden komprimiert und sicher in S3 gespeichert
- ‚úÖ Alte lokale Archive werden automatisch gel√∂scht
- ‚úÖ Alles wird protokolliert

**üì∏ Screenshot 4 einreichen: Cron-Job funktioniert**

Zeige in der **S3-Console**, dass der Cron-Job tats√§chlich automatisch gelaufen ist:

1. Gehe zu S3 ‚Üí dein Bucket ‚Üí logs/
2. Du solltest jetzt **mindestens 2 Dateien** sehen:
   - Eine von deinem manuellen Test (vor Schritt 9)
   - Eine vom automatischen Cron-Job
3. Screenshot sollte zeigen:
   - Beide .tar.gz-Dateien
   - Unterschiedliche Upload-Zeiten (beweist: automatisch!)
   - Dateigr√∂√üe bei beiden

**Bonus:** Zeige zus√§tzlich im Terminal:
```bash
sudo tail -n 20 /var/log/logsync.log
```

Du solltest mehrere Eintr√§ge mit Zeitstempeln sehen - Beweis f√ºr automatische Ausf√ºhrung!

---

## Schritt 10: Optional - Erweiterte Konzepte

### Mehr Traffic erzeugen

Um mehr Logdaten zu generieren, erzeugen wir zus√§tzlichen Traffic.

### Optional: Upload-Pfad pro Host trennen

Wenn du mehrere EC2-Instanzen hast, kannst du die Logs pro Server trennen:

1. √ñffne das Skript:
```bash
sudo nano /opt/logsync/upload_apache_logs.sh
```

2. √Ñndere die Zeile mit `aws s3 sync`:
```bash
# Vorher:
aws s3 sync "${ARCHIVE_DIR}/" "s3://${BUCKET}/logs/" --only-show-errors

# Nachher (mit Hostname):
aws s3 sync "${ARCHIVE_DIR}/" "s3://${BUCKET}/logs/$(hostname)/" --only-show-errors
```

3. Speichern und testen

Jetzt landen die Logs in `s3://dein-bucket/logs/ip-172-31-xx-xx/`

### Mehr Traffic erzeugen

Um mehr Logdaten zu generieren, erzeugen wir zus√§tzlichen Traffic.

### Anleitung

Auf der EC2-Instanz:

1. Traffic von der EC2-Instanz selbst erzeugen:
```bash
for i in {1..100}; do curl -s http://localhost > /dev/null; echo "Request $i sent"; done
```

2. √úberpr√ºfe die Gr√∂√üe der Logdatei:
```bash
ls -lh /var/log/apache2/access.log
```

3. Zeige die letzten 20 Eintr√§ge:
```bash
sudo tail -n 20 /var/log/apache2/access.log
```

4. Von deinem Windows-PC aus (PowerShell):
```powershell
1..50 | ForEach-Object { 
    Invoke-WebRequest -Uri "http://<PUBLIC-IP>" -Method GET -UseBasicParsing
    Write-Host "Request $_ completed"
}
```

Ersetze `<PUBLIC-IP>` mit der Public IP deiner EC2-Instanz.

---

## Schritt 11: Optionale Alternativen

### Alternative ohne AWS CLI (nur AWS Console)

Falls die AWS CLI Probleme macht, kannst du Dateien auch manuell hochladen.

### Variante A: Einfacherer Upload-Befehl (nur heutige Datei)

Wenn du das Skript vereinfachen m√∂chtest, kannst du statt `aws s3 sync` auch `aws s3 cp` verwenden:

1. √ñffne das Skript:
```bash
sudo nano /opt/logsync/upload_apache_logs.sh
```

2. Ersetze die Zeile mit `aws s3 sync` durch:
```bash
# Nur die heutige Datei hochladen (einfacher zu verstehen)
aws s3 cp "${ARCHIVE_DIR}/apache-logs-${STAMP}.tar.gz" "s3://${BUCKET}/logs/"
```

**Unterschied:**
- `sync`: L√§dt alle Dateien im Ordner hoch, die noch nicht in S3 sind
- `cp`: L√§dt nur diese eine Datei hoch

**Vorteil von sync:** Falls ein Upload mal fehlschl√§gt, wird die Datei beim n√§chsten Mal nachgeholt.

### Variante B: Manueller Upload √ºber AWS Console

### Anleitung

1. Auf der EC2-Instanz das Archiv erstellen (ohne Upload):
```bash
sudo tar -czf /tmp/apache-logs-manual.tar.gz -C /var/log/apache2 .
```

2. Archiv herunterladen mit SCP (von deinem Windows-PC):

**PowerShell:**
```powershell
scp -i apache-log-key.pem ubuntu@<PUBLIC-IP>:/tmp/apache-logs-manual.tar.gz ./
```

**WSL:**
```bash
scp -i ~/apache-log-key.pem ubuntu@<PUBLIC-IP>:/tmp/apache-logs-manual.tar.gz ~/
```

3. In AWS Console:
   - Gehe zu S3 ‚Üí dein Bucket ‚Üí logs/
   - Klicke **Upload**
   - Klicke **Add files**
   - W√§hle `apache-logs-manual.tar.gz`
   - Klicke **Upload**

Diese Methode funktioniert immer, ist aber nicht automatisiert.

---

## Schritt 12: IAM-Rolle testen (Mini-Challenge)

Teste, ob die IAM-Berechtigungen wirklich eingeschr√§nkt sind.

### Was lernen wir hier?

Diese Tests zeigen dir, dass **Least Privilege** wirklich funktioniert:
- Dein Server kann nur auf SEINEN Bucket zugreifen
- Nicht auf andere Buckets
- Nicht auf andere AWS-Services

Das ist wichtig f√ºr Sicherheit: Bei einer Kompromittierung ist der Schaden begrenzt!

### Test 1: Policy entfernen und testen

1. In AWS Console ‚Üí IAM ‚Üí Roles ‚Üí EC2-S3-LogUploader
2. Unter Permissions, w√§hle die Policy und klicke **Remove**
3. Auf EC2-Instanz, f√ºhre aus:
```bash
sudo /opt/logsync/upload_apache_logs.sh
```

Erwartetes Ergebnis: Fehler `AccessDenied`

4. Policy wieder hinzuf√ºgen in IAM Console
5. Warte 1-2 Minuten
6. Skript erneut ausf√ºhren - sollte jetzt funktionieren

### Test 2: Anderen Bucket zugreifen

Versuche auf einen Bucket zuzugreifen, f√ºr den du keine Berechtigung hast:

```bash
aws s3 ls s3://aws-logs/
```

Erwartetes Ergebnis: `An error occurred (AccessDenied)`

Das zeigt, dass deine Rolle wirklich nur Zugriff auf deinen spezifischen Bucket hat (Least Privilege Principle).

**Perfekt!** Diese Fehlermeldung ist eigentlich ein **Erfolg** - sie beweist, dass die Sicherheit funktioniert!

---

## Schritt 13: CloudWatch Logs (Optional - Fortgeschritten)

Richte CloudWatch Logs ein, um die Logs auch in AWS CloudWatch zu streamen.

### Anleitung

1. CloudWatch Agent installieren:
```bash
wget https://s3.amazonaws.com/amazoncloudwatch-agent/ubuntu/amd64/latest/amazon-cloudwatch-agent.deb
sudo dpkg -i amazon-cloudwatch-agent.deb
```

2. Konfigurationsdatei erstellen:
```bash
sudo nano /opt/aws/amazon-cloudwatch-agent/etc/config.json
```

Inhalt:
```json
{
  "logs": {
    "logs_collected": {
      "files": {
        "collect_list": [
          {
            "file_path": "/var/log/apache2/access.log",
            "log_group_name": "/aws/ec2/apache-logs",
            "log_stream_name": "{instance_id}-access"
          }
        ]
      }
    }
  }
}
```

3. Agent starten:
```bash
sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl \
  -a fetch-config \
  -m ec2 \
  -s \
  -c file:/opt/aws/amazon-cloudwatch-agent/etc/config.json
```

4. In AWS Console ‚Üí CloudWatch ‚Üí Logs ‚Üí Log groups
   - Du solltest `/aws/ec2/apache-logs` sehen

Dies erfordert zus√§tzliche IAM-Berechtigungen (CloudWatchAgentServerPolicy).

---

## Troubleshooting

### Problem: SSH-Verbindung schl√§gt fehl

**Symptom:** `Connection refused` oder `Connection timed out`

**L√∂sungen:**
1. √úberpr√ºfe, ob die Instanz l√§uft (Instance State: Running)
2. √úberpr√ºfe die Security Group: Port 22 muss f√ºr deine IP ge√∂ffnet sein
3. √úberpr√ºfe, ob du die richtige Public IP verwendest
4. √úberpr√ºfe die Key-Berechtigungen (siehe Schritt 5)

### Problem: AWS CLI funktioniert nicht auf EC2

**Symptom:** `aws: command not found`

**L√∂sung:**
```bash
sudo apt update
sudo apt install -y awscli
aws --version
```

### Problem: 403 AccessDenied beim S3-Upload

**Symptom:** `An error occurred (403) when calling the PutObject operation: Access Denied`

**L√∂sungen:**
1. √úberpr√ºfe, ob die IAM-Rolle der EC2-Instanz zugewiesen ist
   - EC2 Console ‚Üí Instanz ausw√§hlen ‚Üí Actions ‚Üí Security ‚Üí Modify IAM role
2. √úberpr√ºfe den Bucket-Namen im Skript (Tippfehler?)
3. √úberpr√ºfe die IAM-Policy: ARN muss korrekt sein
4. Warte 1-2 Minuten nach Policy-√Ñnderungen (Propagation)

### Problem: Cron-Job l√§uft nicht

**Symptom:** Keine neuen Dateien in S3, keine Eintr√§ge in `/var/log/logsync.log`

**L√∂sungen:**
1. √úberpr√ºfe Cron-Status:
```bash
systemctl status cron
```

2. √úberpr√ºfe Syntax der Cron-Datei:
```bash
cat /etc/cron.d/apache-logsync
```

3. √úberpr√ºfe Berechtigungen:
```bash
ls -l /etc/cron.d/apache-logsync
```
Sollte sein: `-rw-r--r--`

4. Manuell testen:
```bash
sudo /opt/logsync/upload_apache_logs.sh
```

5. Cron-Logs pr√ºfen:
```bash
sudo grep CRON /var/log/syslog | tail -n 20
```

### Problem: Skript schl√§gt fehl mit "command not found"

**Symptom:** `/opt/logsync/upload_apache_logs.sh: line X: aws: command not found`

**L√∂sung:**
Der PATH in der Cron-Datei muss korrekt sein:
```bash
sudo nano /etc/cron.d/apache-logsync
```

Erste Zeilen m√ºssen sein:
```
SHELL=/bin/bash
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
```

### Problem: Bucket-Name enth√§lt Tippfehler

**L√∂sung:**
```bash
sudo nano /opt/logsync/upload_apache_logs.sh
```

Korrigiere die BUCKET-Variable und speichere.

### Problem: Apache-Logs sind leer

**L√∂sung:**
Traffic erzeugen:
```bash
for i in {1..50}; do curl -s http://localhost > /dev/null; done
```

---

## Projekt-Aufr√§umen (nach Abschluss)

Wenn du das Projekt abgeschlossen hast und die Ressourcen nicht mehr ben√∂tigst:

### EC2-Instanz terminieren

1. EC2 Console ‚Üí Instances
2. W√§hle deine Instanz aus
3. Instance state ‚Üí Terminate instance
4. Best√§tige mit "Terminate"

**ACHTUNG:** Alle Daten auf der Instanz gehen verloren!

### S3-Bucket aufr√§umen

**Option 1: Bucket leeren (behalten)**
1. S3 Console ‚Üí dein Bucket ‚Üí logs/
2. W√§hle alle Dateien aus
3. Delete

**Option 2: Bucket komplett l√∂schen**
1. S3 Console ‚Üí dein Bucket
2. Empty bucket (alle Objekte l√∂schen)
3. Dann: Delete bucket

### IAM-Rolle und Policy (optional behalten)

Diese k√∂nnen f√ºr zuk√ºnftige Projekte wiederverwendet werden.

Zum L√∂schen:
1. IAM Console ‚Üí Roles ‚Üí EC2-S3-LogUploader ‚Üí Delete
2. IAM Console ‚Üí Policies ‚Üí S3-LogUpload-Policy ‚Üí Delete

### Security Group (optional behalten)

Kann wiederverwendet werden.

Zum L√∂schen:
1. EC2 Console ‚Üí Security Groups
2. W√§hle `apache-ssh-access`
3. Actions ‚Üí Delete security groups

---

## Zusammenfassung und Reflexion

### Was hast du erreicht?

Herzlichen Gl√ºckwunsch! Du hast ein vollst√§ndig automatisiertes Cloud-Backup-System gebaut. Das ist keine triviale Leistung - viele Unternehmen nutzen genau diese Architektur in Produktion!

**Konkret hast du:**

 **Infrastructure as Code (IaC) verstanden**
- Manuelle Klicks in AWS Console
- In der Praxis: Terraform, CloudFormation
- Reproduzierbar und dokumentiert

 **Cloud-Computing genutzt**
- EC2: Virtuelle Server on-demand
- S3: Unbegrenzter Cloud-Speicher
- Pay-as-you-go statt gro√üe Investition

 **Sicherheits-Best-Practices angewendet**
- IAM-Rollen statt Access Keys
- Least Privilege Principle
- Verschl√ºsselung at rest (SSE-S3)
- Security Groups als Firewall

 **Linux-Administration gelernt**
- Package Management (apt)
- Service Management (systemd)
- Bash Scripting
- Cron Jobs

 **Automatisierung implementiert**
- Zeitgesteuerte Aufgaben
- Fehlerbehandlung
- Logging f√ºr Debugging

### Was w√ºrde in Produktion anders sein?

Diese Demo ist didaktisch vereinfacht. In einem echten Produktions-Setup w√ºrdest du erg√§nzen:

**Sicherheit:**
-  HTTPS (Port 443) statt HTTP (Port 80)
-  SSL-Zertifikat (Let's Encrypt oder AWS Certificate Manager)
-  Private Subnet f√ºr EC2 (nicht direkt im Internet)
-  Bastion Host f√ºr SSH-Zugriff
-  Service-User statt root
-  KMS-Verschl√ºsselung f√ºr S3 (statt SSE-S3)
-  MFA f√ºr AWS Console

**Verf√ºgbarkeit:**
-  Multi-AZ Deployment (mehrere Rechenzentren)
-  Auto Scaling Group (automatisch mehr/weniger Server)
-  Load Balancer (verteilt Traffic)
-  CloudWatch Alarms (Benachrichtigung bei Problemen)
-  Echtzeit-Log-Streaming

**Monitoring:**
-  CloudWatch Dashboards
-  SNS f√ºr Alerts (E-Mail/SMS)
-  Log-Analyse mit CloudWatch Insights
-  Prometheus + Grafana f√ºr Metriken

**Automatisierung:**
-  Infrastructure as Code (Terraform)
-  CI/CD Pipeline (GitHub Actions, GitLab CI)
-  Automated Testing
-  Blue-Green Deployment

**Backup:**
-  Cross-Region Replication (Disaster Recovery)
-  S3 Versioning (Schutz vor versehentlichem L√∂schen)
-  S3 Glacier f√ºr Langzeitarchivierung (g√ºnstiger)
-  Backup-Tests (regelm√§√üig Restore pr√ºfen!)


---

## Checkliste f√ºr diese Selbstlernaufgabe

### Grundlagen verstanden
- [ ] Ich verstehe, was S3 ist und wie Object Storage funktioniert
- [ ] Ich verstehe den Unterschied zwischen IAM-Rollen und Access Keys
- [ ] Ich verstehe, was EC2 ist und wie virtuelle Server funktionieren
- [ ] Ich verstehe, was Security Groups machen

### AWS-Ressourcen erstellt
- [ ] S3-Bucket erstellt (privat, verschl√ºsselt)
- [ ] IAM-Rolle mit Policy erstellt (Least Privilege)
- [ ] Security Group konfiguriert (SSH + HTTP)
- [ ] EC2-Instanz gestartet mit korrekter IAM-Rolle

### Linux-Administration
- [ ] SSH-Verbindung hergestellt
- [ ] Apache Webserver installiert und getestet
- [ ] AWS CLI installiert und getestet
- [ ] Bash-Skript erstellt und verstanden
- [ ] Cron-Job eingerichtet

### Automatisierung funktioniert
- [ ] Skript manuell erfolgreich getestet
- [ ] Upload in S3 verifiziert
- [ ] Cron-Job l√§uft automatisch
- [ ] Logs werden protokolliert

### Screenshots eingereicht
- [ ] **Screenshot 1:** EC2-Instanz l√§uft mit korrekter Konfiguration
- [ ] **Screenshot 2:** Apache l√§uft + AWS CLI installiert
- [ ] **Screenshot 3:** Erste erfolgreiche S3-Upload nach manuellem Test
- [ ] **Screenshot 4:** Mindestens 2 Dateien in S3 (automatischer Cron-Job l√§uft)

### Bonuspunkte (Optional)
- [ ] IAM-Rolle getestet (AccessDenied bei anderem Bucket)
- [ ] Upload-Pfad pro Host getrennt (hostname im Pfad)
- [ ] Mehr Traffic generiert
- [ ] Lifecycle Rule in S3 konfiguriert
- [ ] CloudWatch Logs eingerichtet

---

## H√§ufig gestellte Fragen (FAQ)

**Q: Muss ich die EC2-Instanz laufen lassen?**
A: Nur solange du das Projekt testest. Danach solltest du sie terminieren, um Kosten zu vermeiden.

**Q: Wie lange dauert es, bis der Cron-Job l√§uft?**
A: Cron √ºberpr√ºft jede Minute, ob Jobs anstehen. Dein Job l√§uft dann zu der konfigurierten Zeit (23:55 Uhr).

**Q: Was passiert, wenn der Upload fehlschl√§gt?**
A: Der Fehler wird in `/var/log/logsync.log` protokolliert. Das Archiv bleibt auf der EC2-Instanz und wird beim n√§chsten erfolgreichen Lauf hochgeladen.

**Q: Kann ich mehrere EC2-Instanzen denselben Bucket verwenden lassen?**
A: Ja! √Ñndere im Skript den Upload-Pfad zu: `s3://${BUCKET}/logs/$(hostname)/` - so sind die Logs pro Server getrennt.

**Q: Wie viel kostet das?**
A: Mit t2.micro (Free Tier) und geringen Datenmengen: praktisch kostenlos f√ºr 12 Monate (Free Tier). Danach: wenige Cent pro Monat.

**Q: Kann ich andere Logs auch hochladen?**
A: Ja! Passe einfach die Variable `SRC_DIR` im Skript an, z.B. `/var/log/nginx/` f√ºr Nginx.

**Q: Was ist der Unterschied zwischen `aws s3 sync` und `aws s3 cp`?**
A: 
- `sync` l√§dt nur neue/ge√§nderte Dateien hoch (effizienter, vergleicht Gr√∂√üe und Zeitstempel)
- `cp` l√§dt die angegebene Datei immer hoch, auch wenn sie bereits existiert
- F√ºr dieses Projekt: `sync` ist besser, weil mehrere Archive im Ordner liegen
- Alternative: Wenn du nur die heutige Datei hochladen willst, reicht `cp` (siehe Kommentar im Skript)

---

**Ende der Anleitung**

Bei Fragen oder Problemen, nutze die Troubleshooting-Sektion oder schreibt mir bei Slack!
